<!DOCTYPE html 
    PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" 
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html>
<head>
  <title>SLI | Classes-CS178-Notes / Probability </title>
  <meta http-equiv='Content-Style-Type' content='text/css' />
  <link rel='stylesheet' href='http://sli.ics.uci.edu/pmwiki/pub/skins/custom/pmwiki.css' type='text/css' />
  <!--HTMLHeader--><style type='text/css'><!--
  ul, ol, pre, dl, p { margin-top:0px; margin-bottom:0px; }
  code.escaped { white-space: nowrap; }
  .vspace { margin-top:1.33em; }
  .indent { margin-left:40px; }
  .outdent { margin-left:40px; text-indent:-40px; }
  a.createlinktext { text-decoration:none; border-bottom:1px dotted gray; }
  a.createlink { text-decoration:none; position:relative; top:-0.5em;
    font-weight:bold; font-size:smaller; border-bottom:none; }
  img { border:0px; }
  .editconflict { color:green; 
  font-style:italic; margin-top:1.33em; margin-bottom:1.33em; }

  table.markup { border:2px dotted #ccf; width:90%; }
  td.markup1, td.markup2 { padding-left:10px; padding-right:10px; }
  table.vert td.markup1 { border-bottom:1px solid #ccf; }
  table.horiz td.markup1 { width:23em; border-right:1px solid #ccf; }
  table.markup caption { text-align:left; }
  div.faq p, div.faq pre { margin-left:2em; }
  div.faq p.question { margin:1em 0 0.75em 0; font-weight:bold; }
  div.faqtoc div.faq * { display:none; }
  div.faqtoc div.faq p.question 
    { display:block; font-weight:normal; margin:0.5em 0 0.5em 20px; line-height:normal; }
  div.faqtoc div.faq p.question * { display:inline; }
   
    .frame 
      { border:1px solid #cccccc; padding:4px; background-color:#f9f9f9; }
    .lfloat { float:left; margin-right:0.5em; }
    .rfloat { float:right; margin-left:0.5em; }
a.varlink { text-decoration:none; }

--></style>
  <link href='http://sli.ics.uci.edu/pmwiki/pub}/commentboxplus/commentboxplus.css' rel='stylesheet' type='text/css' />  <meta name='robots' content='index,follow' />

</head>
<body>
<!--PageHeaderFmt-->
  <div id='wikilogo'><a href='http://sli.ics.uci.edu'><img src='/pmwiki/pub/skins/custom/SLI_white.png'
    alt='SLI' border='0' /></a></div>
  <div id='wikihead'>
  <form action='http://sli.ics.uci.edu'>
    <!-- <span class='headnav'><a href='http://sli.ics.uci.edu/Classes-CS178-Notes/RecentChanges'
      accesskey='c'>Recent Changes</a> -</span> --> 
    <input type='hidden' name='n' value='Classes-CS178-Notes.Probability' />
    <input type='hidden' name='action' value='search' />
    <!-- <a href='http://sli.ics.uci.edu/Site/Search'>Search</a>: -->
    <input type='text' name='q' value='' class='inputbox searchbox' />
    <input type='submit' class='inputbutton searchbutton'
      value='Search' />
    <a href='http://sli.ics.uci.edu/Site/Search'>(?)</a>
  </form></div>
<!--/PageHeaderFmt-->
  <table id='wikimid' width='100%' cellspacing='0' cellpadding='0'><tr>
<!--PageLeftFmt-->
      <td id='wikileft' valign='top'>
        <ul><li><a class='wikilink' href='http://sli.ics.uci.edu/Classes/Classes'>Classes</a>
</li><li><a class='wikilink' href='http://sli.ics.uci.edu/Group/Group'>Group</a>
</li><li><a class='wikilink' href='http://sli.ics.uci.edu/Projects/Projects'>Research</a>
</li><li><a class='urllink' href='http://www.ics.uci.edu/~ihler/pubs.html' title='' rel='nofollow'>Publications</a>
</li><li><a class='wikilink' href='http://sli.ics.uci.edu/Code/Code'>Code</a>
</li></ul><div class='vspace'></div><hr />
<div class='vspace'></div>
</td>
<!--/PageLeftFmt-->
      <td id='wikibody' valign='top'>
<!--PageActionFmt-->
        <div id='wikicmds'><ul><li class='browse'><a class='wikilink' href='http://sli.ics.uci.edu/Classes-CS178-Notes/Probability?action=login'>login</a>
</li></ul>
</div>
<!--PageTitleFmt-->
        <div id='wikititle'>
          <div class='pagegroup'><a href='http://sli.ics.uci.edu/Classes-CS178-Notes'>Classes-CS178-Notes</a> /</div>
          <h1 class='pagetitle'>Probability</h1></div>
<!--PageText-->
<div id='wikitext'>
<h2>Review of probability concepts</h2>
<h3>Events</h3>
<p>In probability, an event describes something that may or may not happen; for example, whether it will rain tomorrow, whether I will get the flu, or whether a coin will come up heads.  A probability measure tells us the "size" of these events, measured in terms of how likely they are to occur.  Denoting S the space of all possible events, and A, B individual events (subsets of S), the axioms of probability are
</p><img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/3877084cc3336d5bfe6c8099a4ce15cd.png" />
<div class='vspace'></div><h3>Random variables</h3>
<p>Mostly we will be thinking about probability in terms of variables.  A random variable <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/1bf1f0719d45bbae66f0fef13366c7ec.png" /> may take on values <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/7b96de3b81bd82fdde8b6df73cab7ed6.png" />, each of which constitutes an "event", for example <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/37006a2aed92755d3a0220521e46d896.png" />.  The values of random variables are <em>exhaustive</em> and <em>mutually exclusive</em>, meaning that <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/1bf1f0719d45bbae66f0fef13366c7ec.png" /> cannot equal anything outside of this set (and must take on one of its values), and also cannot equal more than one value in the set (for example, x cannot be simultaneously equal to 1 and 2).  We'll use either <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/045efe45ede704c942a572ca4341436a.png" /> or <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/02e0f245fd5ff33052333661731b1496.png" /> to indicate generic values for random variable <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/1bf1f0719d45bbae66f0fef13366c7ec.png" />.
</p>
<p class='vspace'>This view allows us to specify probabilities involving <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/1bf1f0719d45bbae66f0fef13366c7ec.png" /> in terms of a probability mass function, the probability associated with each of its atomic events <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/37006a2aed92755d3a0220521e46d896.png" />, <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/1da4d4f75b523b0276ba7bc6bff934d5.png" />, etc.  By the axioms of probability, and since the outcomes of x are disjoint events, we have that
</p><img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/3fceec5898d92748e029177c7f5bf670.png" />
<p class='vspace'>Continuous-valued random variables are a bit more subtle, and involve probability <em>density</em> functions rather than mass functions.  In essence, a probability density function is the amount of probability "per unit area" (<img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/7fece3e43c260a4d9aa6fa0227240097.png" />).  An event is defined as the variable <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/1bf1f0719d45bbae66f0fef13366c7ec.png" /> falling into some subset or interval, for example <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/897ea12e874de5a3acdf58f8992c562c.png" />, and its probability mass is defined as the integral of the density over that set:
</p><img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/224361d96b96f19c8b20237f416117a0.png" />
<p>A probability density function can be greater than 1, as long as it is only over a small area; the axioms of probability dictate that the probability mass, or integral of the density, must be less than 1.
</p>
<div class='vspace'></div><hr />
<h2>Common Distributions</h2>
<h3>Bernoulli and Multinomial Distributions</h3>
<p>The most common types of discrete random variable distributions are Bernoulli and multinomial distributions.  The Bernoulli distribution is defined for binary-valued random variables, i.e., <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/50aadf616b4a4a91b58ffa11a3c3b017.png" />, and parameterized by a single parameter <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/51ca345bacfe47bf03a33f39d6830131.png" />.  Multinomial random variables generalize Bernoulli RVs, taking on one of <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/45a3e534b1223a0282926e646207184f.png" /> values and parameterized by a vector of probabilities representing the probability of each outcome.
</p>
<div class='vspace'></div><h3>Gaussian Distributions</h3>
<p>The Gaussian distribution is perhaps the most common distribution for continuous-valued random variables.  The Gaussian probability density function is given by
</p><img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/afce25997a7cfd3bc4d772c183e34611.png" />
<p>Univariate (one-dimensional) Gaussian distributions are parameterized by two scalar numbers, a mean <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/a2f925591f202b15476ff295fd4328c6.png" /> and variance <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/30e55b6472d568a00fa622e8bc14beed.png" /> (sometimes characterized by its square root <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/ff581a3a123af2b561e42e453a5200cf.png" />, the standard deviation).  The mean indicates the center of the Gaussian's characteristic bell-curve shape, and equals the average or expected value of the variable <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/1bf1f0719d45bbae66f0fef13366c7ec.png" />, while the variance or standard deviation indicates its spread, or uncertainty around the mean.
</p>
<p class='vspace'>The multivariate Gaussian is a Gaussian distribution defined for a <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/45a3e534b1223a0282926e646207184f.png" /> dimensional, vector-valued random variable <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/8aff215b31453d0e12234d584696d9dd.png" />, or equivalently a collection of <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/45a3e534b1223a0282926e646207184f.png" /> univariate random variables.  The probability density function is given by
</p><img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/783fb5551cb5aeadf1641d06691ba7b3.png" />
<p>characterized by a mean vector <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/a2f925591f202b15476ff295fd4328c6.png" />, representing a point in the <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/45a3e534b1223a0282926e646207184f.png" /> dimensional space, and a <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/a5dfc7cd7db8a9db8cd1fbce913eae74.png" /> covariance matrix <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/a5e8a1fdcd3c9ea302bb85b898c42e46.png" /> representing the shape and spread of the data.  The dependence on <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/6cb53fbb14261422b799d12762e0af0e.png" /> (in the exponential) is a quadratic form, and the shape of the equally-probable contour lines are ellipses.
</p>
<p class='vspace'>Just as the square root of the variance was helpful in representing the spread in one dimension, the matrix square root can help us understand the shape and size of the uncertainty in <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/45a3e534b1223a0282926e646207184f.png" /> dimensions.  We can represent <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/fec2260d0f679513f47163a36294085b.png" /> using its eigenvector decomposition, so that <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/2ec8cb121139c5a5a919cdf776f41ec2.png" /> is a diagonal matrix of eigenvalues and <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/db9a4d69e6084262f774ea1ea807c138.png" /> is a unitary matrix.  Then, the generalization of the standard deviation is <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/292c232fd1cda30c77149e1641494e3f.png" />, where <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/a695efc569d7fffcfbd01f178cbd0451.png" /> represents a scaling and <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/7c8f8a29f55f3d3ffddab2ba88153a72.png" /> a rotation.
</p>
<p class='vspace'>This helps us see two special cases of the Gaussian distribution.  A fully general covariance matrix has ellipsoidal uncertainty shapes; a <em>diagonal</em> covariance looks like an axis-aligned ellipse (no rotation), and a <em>spherical</em> Gaussian has a "scalar" covariance (a scalar times an identity matrix, or diagonal with all the same value).
</p>
<p class='vspace'>We can draw samples from a multivariate Gaussian easily using this construction, by first sampling from a unit-variance
</p>
<div class='vspace'></div><ul><li>Understanding and generating multivariate Gaussian data
<ul><li>Shape, in terms of eigenvectors of Sigma
</li><li>Full, diagonal, or spherical Gaussian distributions
</li><li>Generating Gaussian samples
<ul><li>Spherical samples
</li><li>Scaling
</li><li>Rotation 
</li></ul></li></ul></li></ul><div class='vspace'></div><hr />
<h2>Density estimation</h2>
<p>Since machine learning is primarily concerned with <em>adapting</em> to observed data, most of our probability models are likely to be estimated from data.
</p>
<div class='vspace'></div><h3>Histograms</h3>
<p>A histogram is a simple method of estimating and visualizing a probability density function.  We bin the observed data and report the fraction of data falling into each bin.  This can be interpreted as a piecewise-constant estimator of the probability density function.
</p>
<div class='vspace'></div><h3>Maximum likelihood methods</h3>
<div class='vspace'></div><h2>Overfitting in density estimation</h2>
<ul><li>As in regression, it is possible to <em>overfit</em> to the observed data
<ul><li>Typically an effect of a complex density estimate given limited data
</li><li>
</li></ul></li></ul><div class='vspace'></div><hr />
<h2>Independence and Conditional Independence</h2>
<p>When two random events are <em>independent</em>, it greatly simplifies their probabilities.  Independent events do not influence each others' outcome, e.g., if two events A,B are independent, then knowing that A occurred has no influence on the probability of B occurring: <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/0de094c747183188c119e4461453de8f.png" />.  In general, this means that the joint probabilities factor into a product:<br /><img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/b7fe382e1a2a01de3c1cf27e8fdf5217.png" />
</p>
<p class='vspace'>However, in practice the variables we are interested in <em>are</em> related to one another somehow, and so are not completely independent.  A more useful type of independence relationship is <em>conditional</em> independence, in which two or more variables influence one another only through some intermediary variable.  For example, our two events A,B may be independent of one another once we control for some cause C: <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/00145c33067f175c6eb1573fa7bdbcfb.png" />.  (This is an <em>assumption</em> about the structure of the joint distribution.)
</p>
<div class='vspace'></div>
</div>

      </td>
    </tr></table>
<!--PageFooterFmt-->
  <div id='wikifoot'>
    <div class='footnav' style='float:left'> Last modified March 27, 2011, at 02:30 PM</div>
    <div class='footnav' style='float:right; text-align:right'>
    <a href="http://www.ics.uci.edu">Bren School of Information and Computer Science</a><br>
    <a href="http://www.uci.edu">University of California, Irvine</a>
    </div>
  </div>
<!--HTMLFooter--><script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(["_setAccount", "UA-24148957-2"]);
	_gaq.push(["_trackPageview"]);
	(function() {
	  var ga = document.createElement("script"); ga.type = "text/javascript"; ga.async = true;
	  ga.src = ("https:" == document.location.protocol ? "https://ssl" : "http://www") + ".google-analytics.com/ga.js";
	  var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(ga, s);
	  })();
</script>
</body>
</html>

