<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Qiang Liu</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Qiang Liu</div>
<div class="menu-item"><a href="index.html" class="current">home</a></div>
<div class="menu-item"><a href="publication.html">Publications</a></div>
<div class="menu-item"><a href="PDF/qiang_cv.pdf">CV&nbsp;(PDF)</a></div>
<div class="menu-item"><a href="TA_CS178.html">TA&nbsp;(CS178)</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Qiang Liu</h1>
</div>
<table class="imgtable"><tr><td>
<a href="IMGLINKTARGET"><img src="pics/QiangMS.jpg" alt="alt text" width="153.8" height="213.8" /></a>&nbsp;</td>
<td align="left"><p>Qiang Liu <br />
Ph.D. Candidate <br />
Advisor: <a href="http://www.ics.uci.edu/~ihler/">Prof. Alexander Ihler</a><br />
<a href="http://www.ics.uci.edu/">Information &amp; Computer Science</a> <br />
<a href="http://www.uci.edu/">University of California at Irvine</a> <br />
<a href="mailto:qliu@uci.edu">qliu1(at)uci.edu</a></p>
<p><font size="2" ><i> ("Qiang" sounds like "Chee-ah-ng", and "Liu" as "l-yo")<i></font>  <br /></p>
<p><br />
<br />
I am in my 5th year of my PhD. I am currently supported by a 
<a href="http://research.microsoft.com/en-us/collaboration/global/northam/northam-fellows.aspx">Microsoft Research PhD fellowship</a>.</p>
</td></tr></table>
<p><br /></p>
<p><b> <font color="red">New.      </font> </b></p>
<p>I am co-organizing a NIPS&rsquo;13 workshop on <a href="http://www.ics.uci.edu/~qliu1/nips13_workshop/">Crowdsourcing: Theory, Algorithms and Applications</a>.  <br />
<a href="http://www.ics.uci.edu/~qliu1/MLcrowd_ICML_workshop/">Here</a> is a related workshop I organized previously.</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </p>
<p><!-- I co-organized an ICML'13 workshop on [http://www.ics.uci.edu/~qliu1/MLcrowd_ICML_workshop/ Machine Learning Meets Crowdsourcing].  --></p>
<h2>Research</h2>
<p>My research area is machine learning and statistics, with interests spreading over the pipeline of 
<a href="http://www.ics.uci.edu/~qliu1/#crowdsourcing">data collection</a> (mainly <a href="http://www.ics.uci.edu/~qliu1/#crowdsourcing">crowdsourcing</a>), 
<a href="http://www.ics.uci.edu/~qliu1/#learning">learning</a>, 
<a href="http://www.ics.uci.edu/~qliu1/#inference">inference</a>, 
<a href="http://www.ics.uci.edu/~qliu1/#decisionmaking">decision making</a>, 
and various <a href="http://www.ics.uci.edu/~qliu1/#application">applications</a> 
under the framework of probabilistic graphical models.</p>
<div class="infoblock">
<div class="blockcontent">
<p><b><a name='crowdsourcing'> Crowdsourcing</a>.</b> 
All machine learning processes start from data collection. Crowdsourcing is a modern approach to collect large amounts of labeled data by hiring anonymous workers through online platforms such as Amazon Mechanical Turk. 
Unfortunately, the crowdsourced workers are often unreliable and uncontrollable, raising many challenging computational questions, such as how to aggregate labels from workers with different expertise, how to combine and balance noisy (but cheap) crowdsourced labels and accurate (but expensive) expert labels, and how to crowdsource complicated objectives such as protein structures. </p>
<ul>
<li><p>We reform the problem of aggregating crowdsourced labels into a standard inference problem on a factor graph, which we solve using a class of variational inference algorithms. We show that both the na&iuml;ve majority voting method and a previous algorithm by Karger et al. 2012 are special cases of one of our belief-propagation-type algorithms with special priors. We demonstrate significant improvement on the performance by using better priors, see <a href = "http://www.ics.uci.edu/~qliu1/PDF/crowdsrc_nips12.pdf">NIPS2012</a>, <a href = "http://www.ics.uci.edu/~qliu1/codes/crowd_tool.zip">code.</a> </p>
</li>
</ul>
<ul>
<li><p>Control items with known answers can be used to evaluate workers&rsquo; performance, and hence improve the combined results on the target items with unknown answers. This raises the problem of how many control items to use when the total number of items each workers can answer is limited: more control items evaluates the workers better, but leaves fewer resources for the target items that are of direct interest, and vice versa. We perform theoretical analysis and provide surprisingly simple answers for this problem, see <a href = "http://www.ics.uci.edu/~qliu1/PDF/main_nips2013.pdf">here.</a></p>
</li>
</ul>
<ul>
<li><p>A preliminary thought on combining structured labels such as the protein folding, see <a href = "http://www.ics.uci.edu/~qliu1/MLcrowd_ICML_workshop/Papers/ActivePaper12.pdf">here.</a></p>
</li>
</ul>
<p><!-- We study the problem of estimating continuous quantities, such as prices, probabilities, and point spreads, using a crowdsourcing approach. A challenging aspect of combining the crowd’s answers is that workers’ reliabilities and biases are usu- ally unknown and highly diverse. 
We give theoretical results for this problem under different scenarios, and provide a simple rule of thumb for crowdsourcing practitioners. As a byproduct, we also provide theoretical analysis of the accuracy of different consensus methods. --></p>
</div></div>
<div class="infoblock">
<div class="blockcontent">
<p><b><a name='learning'> Learning</a>.</b> 
Learning refers to constructing probabilistic models from empirical data, either to estimate the model parameters with predefined model structures, or even to estimate the model structures solely from data? I am interested in developing efficient, possibly distributed, learning algorithms, that perform well on real world data.</p>
<ul>
<li><p>Here is an efficient distributed learning algorithm based on smartly combining local estimators defined by pseudo-likelihood components: 
<a href="http://www.ics.uci.edu/~qliu1/PDF/distributed_mple_fit.pdf">ICML2012</a>.</p>
</li>
</ul>
<ul>
<li><p>Here is a structure learning algorithm for recovering scale-free networks, thought to appear commonly in the real world: 
<a href="http://www.ics.uci.edu/~qliu1/PDF/aistats11.pdf">AISTATS2011</a> (notable paper award).</p>
</li>
</ul>
<ul>
<li><p>Here are some earlier works on contrastive divergence and MCMC-MLE: 
<a href="http://www.ics.uci.edu/~ihler/papers/icml10.pdf">ICML2010</a>; 
<a href="http://www.ics.uci.edu/~ihler/papers/aistats10.pdf">AISTATS2010</a>.</p>
</li>
</ul>
</div></div>
<div class="infoblock">
<div class="blockcontent">
<p><b><a name='inference'> Inference</a>.</b>
With given graphical models, either handcrafted or learned from data, inference refers to answering queries, such as marginal probability (or partition function), maximum a posteriori (MAP) estimation, or marginal MAP, the hybrid of marginalization and MAP. I am interested in developing efficient inference algorithms, mostly based on variational methods and in the form of belief-propagation-like message passing algorithms.</p>
<ul>
<li><p>Marginal MAP is notoriously difficult even on tree-structured graphs. We developed a general variational dual representation for marginal MAP, and propose a set of variational approximation algorithms, including an interesting &ldquo;mixed-product&rdquo; BP that is a hybrid of max-product, sum-product and a special &ldquo;argmax-product&rdquo; message updates, and a convergent proximal point algorithm that works by iteratively solving pure marginalization tasks. See 
<a href="http://www.ics.uci.edu/~qliu1/PDF/marginalMAP_jmlr.pdf">JMLR2013</a>; 
<a href="http://www.ics.uci.edu/~qliu1/PDF/uai11.pdf">UAI2011</a>
<a href="http://www.ics.uci.edu/~qliu1/PDF/marginalMAP_only.pdf">(Slides)</a>.</p>
</li>
</ul>
<ul>
<li><p>We proposed an efficient approximate inference algorithm for calculating the log-partition function that unifies Rina Dechter's &ldquo;one-pass&rdquo; mini-bucket algorithm with iterative variational algorithms, such as tree reweighted BP. Our method inherits the advantages of both, and easily scales to large clique sizes. Our algorithm can provide both upper and lower bounds for the log-partition function. See <a href="http://www.ics.uci.edu/~qliu1/PDF/icml11.pdf">ICML2011</a>.</p>
</li>
</ul>
<ul>
<li><p>Tree reweighted BP provides an upper bound on the log-partition function, while na&iuml;ve mean field and structured mean field give lower bounds. We show that tree reweighted BP provably gives a lower bound if its weights are set to take negative values in a particular way. We also show that such &ldquo;negative&rdquo; tree reweighted BP reduces to structured mean field as the weights approach infinity. For the full story, see <a href="http://www.ics.uci.edu/~qliu1/PDF/neg_weights.pdf">UAI2010</a>.</p>
</li>
</ul>
</div></div>
<div class="infoblock">
<div class="blockcontent">
<p><b><a name='decisionmaking'> Structured decision making</a>.</b> 
In practice, we often need to take a sequence of actions to achieve a predefined goal, usually under uncertain environments where information is observed sequentially and interactively as we progress. Decision networks (also called influence diagrams) are graphical model style representations of such structured decision making problems under uncertainty. Just like Bayesian networks generalize Markov chains or hidden Markov chains, decision networks generalize Markov decision processes (MDP), or partially observable decision processes (POMDP). Unfortunately, the problem of finding the optimal actions for decision networks is much more challenging than answering queries on Bayesian networks, especially in cases where limited information is observed or where multi-agent cooperation is required (such as in robot soccer games).</p>
<ul>
<li><p>We extend the powerful variational inference framework for solving decision networks, based on which we propose an efficient BP-type algorithm and a convergent proximal point algorithm. Our framework enables us to translate basically any variational algorithm to solve influence diagrams. See <a href = "http://www.ics.uci.edu/~qliu1/PDF/uai12_meu.pdf">UAI2012.</a></p>
</li>
</ul>
</div></div>
<div class="infoblock">
<div class="blockcontent">
<p><b><a name='application'> Applications</a>.</b>  I am interested in applying these machine learning methods in many application areas.</p>
<ul>
<li><p>Natural language processing:</p>
<ul>
<li><p>How well can computers solve the SAT sentence completion question? This is the work I involved when I was interning in Microsoft Research Redmond, <a href = "http://research.microsoft.com/apps/pubs/default.aspx?id=163344">ACL2012.</a></p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>Sensor networks:</p>
<ul>
<li><p>Here is a distributed algorithm for learning parameters in sensor networks: <a href = "http://www.ics.uci.edu/~qliu1/PDF/distributed_mple_fit.pdf"> ICML2012</a>.</p>
</li>
<li><p>My algorithm for solving influence diagrams provides a powerful way to design optimal decentralized detection networks: <a href = "http://www.ics.uci.edu/~qliu1/PDF/uai12_meu.pdf"> UAI2012</a>.</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>Bioinformatics:</p>
<ul>
<li><p><a href = "http://www.pnas.org/content/109/29/11758.long"> PNAS2012</a>; <a href = "http://www.ics.uci.edu/~qliu1/PDF/aistats11.pdf"> AISTATS2011</a>;   <a href = "http://bioinformatics.oxfordjournals.org/content/26/6/770.full.pdf+html">Bioinformatics2010 </a>. </p>
</li>
</ul>

</li>
</ul>
</div></div>
<p><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /></p>
<p>
<div id="clustrmaps-widget"></div><script type="text/javascript">var _clustrmaps = {'url' : 'http://www.ics.uci.edu/~qliu1', 'user' : 1080620, 'server' : '4', 'id' : 'clustrmaps-widget', 'version' : 1, 'date' : '2013-02-23', 'lang' : 'en', 'corners' : 'square' };(function (){ var s = document.createElement('script'); s.type = 'text/javascript'; s.async = true; s.src = 'http://www4.clustrmaps.com/counter/map.js'; var x = document.getElementsByTagName('script')[0]; x.parentNode.insertBefore(s, x);})();</script><noscript><a href="http://www4.clustrmaps.com/user/294107d2c"><img src="http://www4.clustrmaps.com/stats/maps-no_clusters/www.ics.uci.edu-~qliu1-thumb.jpg" alt="Locations of visitors to this page" /></a></noscript>
</p>
<div id="footer">
<div id="footer-text">
Page generated 2013-12-04 13:30:02 PST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
(<a href="index.jemdoc">source</a>)
</div>
</div>
</td>
</tr>
</table>
</body>
</html>

