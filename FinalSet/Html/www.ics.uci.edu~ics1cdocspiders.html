<HTML>
<HEAD>
<base href="http://www.rpi.edu/~decemj/cmc/mag/1994/sep/spiders.html">
   <TITLE>New Spiders Roam the Web</TITLE>
   <LINK REV=made HREF="mailto:decemj@rpi.edu">
</HEAD>
<BODY>
<i><a href="../../current/toc.html">Computer-Mediated 
Communication Magazine</a></i> / 
Volume 1, Number 5 / September 1, 1994 / Page 3<p>

<HR>

<H1>
New Spiders Roam the Web
</H1>

by 
<a href="http://www.rpi.edu/~decemj/index.html">John December</a> 
(decemj@rpi.edu)
<p>

THE WEB (August 28) 
Two newer, smarter tools for finding and indexing resources on the 
Web have been released this summer. The 
<a href="http://www.cmu.edu/">Carnegie Mellon University</a>'s
<a href="http://www.mt.cs.cmu.edu/cmt/CMT-home.html">Center for 
Machine Translation</a> announced the public availability of its 
<a href="http://lycos.cs.cmu.edu/">Lycos (TM)
WWW search engine</a> on August 12th, and 
the <a href="http://rd.cs.colorado.edu/~schwartz/IRTF.html">Internet Research 
Task Force Research Group on Resource Discovery</a>'s 
<a href="http://rd.cs.colorado.edu/harvest/Home.html">Harvest System</a>
has been presented in several papers during the summer.
Both systems are now in place for public use.
<P>

The Lycos and Harvest systems attack a problem that 
has plagued many information spaces before the Web--how 
can a user find resources related to a topic or locate a 
specific resource?   
In 
<a href="http://hoohoo.ncsa.uiuc.edu/ftp-interface.html">ftpspace</a>, 
there's <a href="http://web.nexor.co.uk/archie.html">archie</a>;
in <a href="gopher://gopher.micro.umn.edu/1">gopherspace</a>, 
there's <a href="gopher://gopher.unr.edu/11/veronica">veronica</a>.
For the Web, there is a 
variety of Web 
<a href="http://web.nexor.co.uk/mak/doc/robots/robots.html">robots, 
wanderers, and spiders</a> that have been crawling through
the Web and collecting information about what they find. 
Oliver McBryan's 
<a href="http://www.cs.colorado.edu/home/mcbryan/WWWW.html">World-Wide 
Web Worm</a>, released in March, was a very early ancestor
to the newer species of spiders on the Web today.
The Worm collected a database of over 100,000 resources and still provides
the user with a search interface to its database (current to March 7, 1994).
Both Lycos and Harvest build on the 
Worm's techniques, provide more current databases, and 
collect them in a more efficient manner.  
<P>

<H2>Lycos</H2>

In an email interview, 
<a href="http://fuzine.mt.cs.cmu.edu/mlm/home.html">Dr. 
Michael L. Mauldin</a>, a developer of Lycos, 
described the spider's unique features.  
Lycos' software ancestry is from 
a program called "Longlegs" written by 
<a href="http://thule.mt.cs.cmu.edu:8001/jrrl-space/home-page.html">John 
Leavitt</a> 
and <a href="http://www.mt.cs.cmu.edu/ehn/release/">Eric Nyberg</a>, and 
the term "Lycos" comes from 
the arachnid family <i>Lycosidae</I>, which are large ground
spiders that are very speedy and active at night, catching their
prey by pursuit rather than in a web.
Lycos lives up to its name--rather than catching its "prey" (URLs
on a server) in a massive single-server sweeps, 
Lycos uses an innovative, 
probabilistic scheme to 
skip from sever to server in Webspace. 
<P>

The secret of Lycos' search technique lies in random choices
tempered by preferences.   Lycos starts with a given URL and 
collects information from the resource, including:
<UL>
<LI>Title 
<LI>Headings and Subheadings 
<LI>100 most "weighty" words (using an algorithm which considers
	word placement and frequencies, among other factors)
<LI>First 20 lines 
<LI>Size in bytes 
<LI>Number of words 
</UL>
Lycos then adds the URL references in the resource
to its queue.  To choose the next document to explore, 
Lycos makes a random choice 
(among the http, gopher, and ftp references) 
with built-in "preferences" for documents that 
have multiple links into them (popular documents) and 
a slight preference for shorter URLs (to keep the database
oriented to the Web's "top").
<P>

<HR>
<BLOCKQUOTE>
<B>
While many early Web spiders 
infested a particular server with a large number of rapid, 
sequential accesses, Lycos behaves.  
</B>
</BLOCKQUOTE>
<HR>
First, Lycos'
random-search behavior avoids the "multiple-hit" problem.
Second, Lycos complies with 
<a href="http://web.nexor.co.uk/mak/doc/robots/norobots.html">the 
standard for robot exclusion</a> 
to keep unwanted robots off WWW servers, 
and identifies itself as 
'Lycos' when crawling, so that webmasters 
can know when Lycos has hit their server.
<P>

With more than 634,000 references in its database as of 
the end of August, Lycos offers a huge database to locate
documents matching a given query.  The 
<a href="http://lycos.cs.cmu.edu/cgi-bin/pursuit/">search interface</a> 
provides a way for users to find documents that contain 
references to a keyword, and to 
examine a document outline, keyword list and
an excerpt.  In this way, Lycos enables the user to determine
if a document might be valuable <i>without having to retrieve it</i>.
According to Dr. Mauldin, plans are in the works for
allowing users to register pages and for other kinds of searching
schemes.
Another related project underway is 
<a href="http://thule.mt.cs.cmu.edu:8001/jrrl-space/webants.html">
WebAnts</a> aimed at creating <i>cooperating</i> explorers, so 
that an individual spider doesn't have to do all the work of 
finding things on the Web or duplicate other spiders' efforts.

<H2>The Harvest Project</H2>

The <a href="http://rd.cs.colorado.edu/harvest/">The Harvest 
Information Discovery and Access System</a> reaches beyond being 
merely a spider, but involves a series of subsystems to create
an efficient, flexible, and scalable way to locate information.
Harvest is an ambitious project to provide a way to create indexes and 
provide for efficient use of servers.
Work on its development has been supported primarily by 
<a href="http://ftp.arpa.mil/">Advanced Research Projects Agency</a>,
with other support from
<a href="http://web.fie.com/web/fed/afr/">Air Force Office of Scientific 
Research (AFOSR)</a>, Hughes, 
<a href="gopher://stis.nsf.gov/11">National Science Foundation</a>,
and <a href="http://www.sun.com">Sun</a>.
Harvest is being 
designed and built by the
<a href="http://rd.cs.colorado.edu/~schwartz/IRTF.html">Internet 
Research Task Force Research Group on Resource Discovery</a>.
<P>

The philosophy behind the Harvest system is that it gathers information
about Internet resources and customizes views into what is "harvested."
According to developer
<a href="http://rd.cs.colorado.edu/~schwartz/Home.html">Mike Schwartz,</a> 
"Harvest is much more than just a 'spider.'  
It's intended to be a
scalable form of infrastructure for building and distributing content,
indexing information, as well as for accessing Web information."
The complete capabilities of Harvest are beyond the scope of this 
news article; for further information, the reader is directed to 
<a href="http://bruno.cs.colorado.edu/harvest/">The Harvest 
Information Discovery and Access System web page</a>.
<P>

Harvest consists of several subsystems.
A Gatherer collects indexing information and 
a <a href="http://rd.cs.colorado.edu/brokers/">Broker</a>
provides a flexible interface to this information.
A user can access a variety of collections of 
documents.  The 
<a href="http://rd.cs.colorado.edu/brokers/www-home-pages/query.html">Harvest WWW Broker</a>, for example, includes content summaries of more than
7,000 Web pages.  This databasa has a very flexible interface, providing
search queries based on author, keyword, title, or URL-reference.
While the Harvest database (the 
WWW pages) isn't yet as extensive as other spiders', its potential 
for efficiently collecting a large amount is great.
<P>

Other subsystems further refine Harvest's capabilities.
Subsystems for Indexing/Searching provides a ways for 
for a variety of search engines to be used.  For example, 
<a href="http://glimpse.cs.arizona.edu:1994/">Glimpse</a>
supports very rapid space-efficient searches with interactive 
queries while 
<a href="http://canopus.cse.psu.edu/NEBFS/nebula.html">Nebula</a>
provides fast searches for 
more complex queries.  Another Harvest subsystem, 
a Replicator, provides a way to mirror information the Brokers have 
and an Object Cache meets the demand for managing 
networked information by providing the capability to locate
the fastest-responding server to a query.
<P>

While spiders like the Worm could successfully crawl through 
Webspace in first part of 1994, 
the rapid increase in the amount of information on the Web since 
then make this same crawl difficult for the older spiders.
Harvest's systems and subsystems are extensive and provide
for efficient, flexible operation, and its design 
addresses the very important issue of scalability.  
Similarly, the 
<a href="http://thule.mt.cs.cmu.edu:8001/jrrl-space/webants.html">Web Ants</a> 
project addresses this scalability issue through its vision of 
cooperating spiders crawling through the Web.
The promise for the future 
is that systems like Harvest and Lycos will provide
users with increasingly efficient ways to locate information on 
the Nets.
&#164
<P>

<HR>

<i>
<a href="toc.html">This Issue </a> /
<a href="../../index.html">Index</a>
</i>
<p>

</BODY>
</HTML>

