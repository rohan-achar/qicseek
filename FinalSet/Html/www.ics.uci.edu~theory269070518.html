<!DOCTYPE html PUBLIC "-//IETF//DTD HTML 2.0//EN">
<html>
<head>
<title>Theory Seminar, May 18, 2007</title>
</head>
<body>
<a href="/~theory/"><img src="/~theory/logo/shortTheory.gif" width="
521" height="82" border="0" alt="ICS Theory Group"></a>

<h2>CompSci 269S, Spring 2007: Theory Seminar</h2>

<h3>May 18, 2007, in Bren Hall 1423</h3>

<h1>Guaranteed-Accurate Floating-Point Summation</h1>

<h2>Yong-Kang Zhu</h2>

<p>Abstract:</p>

<p>Summation of many floating-point numbers introduces many rounding
errors.  When the data are ill-conditioned, the computed sum can be far
from the exact sum, being overwhelmed with roundoff error.  Dekker and
Knuth independently invented an algorithm called <code>AddTwo</code>
that adds two floating-point numbers together while simultaneously
computing the exact roundoff error. Based on <code>AddTwo</code>, we
present an algorithm, <code>SimpleSum</code>, which repeatedly calls
<code>AddTwo</code>. Given an array <i>x</i> of summands we prove that,
after finite iterations of the outer loop, <code>SimpleSum</code>
reaches a steady state in which <i>x</i> is sorted by increasing
magnitude, non-overlapping mantissas, and fixed exponents, such that
from <i>x<sub>n</sub></i> to <i>x</i><sub>1</sub> each element contains
a smaller-and smaller component of the roundoff error.  That is, the
converged state of the array contains a full, exact and optimally
compact representation of the sum of the original array. The algorithm
works for any floating-point base.</p>

<p>Then, we will briefly outline a more sophisticated algorithm based
on the same principle, that runs in practice in only two "for"
loops, on average, and a "hybrid" that uses some other ideas to
run even faster.</p>

</body>
</html>

