<html xmlns:v="urn:schemas-microsoft-com:vml"
xmlns:o="urn:schemas-microsoft-com:office:office"
xmlns:w="urn:schemas-microsoft-com:office:word"
xmlns:st1="urn:schemas-microsoft-com:office:smarttags"
xmlns="http://www.w3.org/TR/REC-html40">

<head>
<meta http-equiv=Content-Type content="text/html; charset=us-ascii">
<meta name=ProgId content=Word.Document>
<meta name=Generator content="Microsoft Word 11">
<meta name=Originator content="Microsoft Word 11">
<link rel=File-List href="GraphicalModels_files/filelist.xml">
<link rel=Edit-Time-Data href="GraphicalModels_files/editdata.mso">
<!--[if !mso]>
<style>
v\:* {behavior:url(#default#VML);}
o\:* {behavior:url(#default#VML);}
w\:* {behavior:url(#default#VML);}
.shape {behavior:url(#default#VML);}
</style>
<![endif]-->
<title>Untitled Document</title>
<o:SmartTagType namespaceuri="urn:schemas-microsoft-com:office:smarttags"
 name="place"/>
<o:SmartTagType namespaceuri="urn:schemas-microsoft-com:office:smarttags"
 name="City"/>
<!--[if gte mso 9]><xml>
 <o:DocumentProperties>
  <o:Author>Welling</o:Author>
  <o:Template>Normal</o:Template>
  <o:LastAuthor>Welling</o:LastAuthor>
  <o:Revision>9</o:Revision>
  <o:TotalTime>78</o:TotalTime>
  <o:Created>2006-03-31T23:49:00Z</o:Created>
  <o:LastSaved>2006-05-03T17:49:00Z</o:LastSaved>
  <o:Pages>1</o:Pages>
  <o:Words>1422</o:Words>
  <o:Characters>8112</o:Characters>
  <o:Company> UCI</o:Company>
  <o:Lines>67</o:Lines>
  <o:Paragraphs>19</o:Paragraphs>
  <o:CharactersWithSpaces>9515</o:CharactersWithSpaces>
  <o:Version>11.5606</o:Version>
 </o:DocumentProperties>
</xml><![endif]--><!--[if gte mso 9]><xml>
 <w:WordDocument>
  <w:Zoom>125</w:Zoom>
  <w:ValidateAgainstSchemas/>
  <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid>
  <w:IgnoreMixedContent>false</w:IgnoreMixedContent>
  <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText>
  <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel>
 </w:WordDocument>
</xml><![endif]--><!--[if gte mso 9]><xml>
 <w:LatentStyles DefLockedState="false" LatentStyleCount="156">
 </w:LatentStyles>
</xml><![endif]--><!--[if !mso]><object
 classid="clsid:38481807-CA0E-42D2-BF39-B33AF135CC4D" id=ieooui></object>
<style>
st1\:*{behavior:url(#ieooui) }
</style>
<![endif]-->
<style>
<!--
 /* Font Definitions */
 @font-face
	{font-family:Tahoma;
	panose-1:2 11 6 4 3 5 4 4 2 4;
	mso-font-charset:0;
	mso-generic-font-family:swiss;
	mso-font-pitch:variable;
	mso-font-signature:1627421319 -2147483648 8 0 66047 0;}
 /* Style Definitions */
 p.MsoNormal, li.MsoNormal, div.MsoNormal
	{mso-style-parent:"";
	margin:0in;
	margin-bottom:.0001pt;
	mso-pagination:widow-orphan;
	font-size:12.0pt;
	font-family:"Times New Roman";
	mso-fareast-font-family:"Times New Roman";}
h2
	{mso-margin-top-alt:auto;
	margin-right:0in;
	mso-margin-bottom-alt:auto;
	margin-left:0in;
	mso-pagination:widow-orphan;
	mso-outline-level:2;
	font-size:18.0pt;
	font-family:"Times New Roman";}
a:link, span.MsoHyperlink
	{color:blue;
	text-decoration:underline;
	text-underline:single;}
a:visited, span.MsoHyperlinkFollowed
	{color:blue;
	text-decoration:underline;
	text-underline:single;}
@page Section1
	{size:8.5in 11.0in;
	margin:1.0in 1.25in 1.0in 1.25in;
	mso-header-margin:.5in;
	mso-footer-margin:.5in;
	mso-paper-source:0;}
div.Section1
	{page:Section1;}
-->
</style>
<!--[if gte mso 10]>
<style>
 /* Style Definitions */
 table.MsoNormalTable
	{mso-style-name:"Table Normal";
	mso-tstyle-rowband-size:0;
	mso-tstyle-colband-size:0;
	mso-style-noshow:yes;
	mso-style-parent:"";
	mso-padding-alt:0in 5.4pt 0in 5.4pt;
	mso-para-margin:0in;
	mso-para-margin-bottom:.0001pt;
	mso-pagination:widow-orphan;
	font-size:10.0pt;
	font-family:"Times New Roman";
	mso-ansi-language:#0400;
	mso-fareast-language:#0400;
	mso-bidi-language:#0400;}
</style>
<![endif]--><!--[if gte mso 9]><xml>
 <o:shapedefaults v:ext="edit" spidmax="6146"/>
</xml><![endif]--><!--[if gte mso 9]><xml>
 <o:shapelayout v:ext="edit">
  <o:idmap v:ext="edit" data="1"/>
 </o:shapelayout></xml><![endif]-->
</head>

<body bgcolor="#CCCCCC" background="../../background.gif" lang=EN-US link=blue
vlink=blue style='tab-interval:.5in'>

<div class=Section1>

<h2><span style='font-size:10.0pt;font-family:Tahoma;color:red'>Learning in
Graphical Models &#8211; spring 2006</span><span style='font-size:10.0pt'><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt'>ICS: 274B<br>
Instructor: Max Welling</span><span style='color:black'>&nbsp;</span><span
style='font-size:10.0pt'><o:p></o:p></span></h2>

<table class=MsoNormalTable border=0 cellspacing=0 cellpadding=0 width=311
 style='width:233.1pt;mso-cellspacing:0in;mso-padding-alt:2.25pt 2.25pt 2.25pt 2.25pt'>
 <tr style='mso-yfti-irow:0;mso-yfti-firstrow:yes'>
  <td style='background:#E7E7E7;padding:2.25pt 2.25pt 2.25pt 2.25pt'>
  <p class=MsoNormal align=center style='text-align:center'><b><span
  style='font-size:8.5pt;color:black'>Code<o:p></o:p></span></b></p>
  </td>
  <td style='background:#E7E7E7;padding:2.25pt 2.25pt 2.25pt 2.25pt'>
  <p class=MsoNormal align=center style='text-align:center'><b><span
  style='font-size:8.5pt;color:black'>Typ<o:p></o:p></span></b></p>
  </td>
  <td style='background:#E7E7E7;padding:2.25pt 2.25pt 2.25pt 2.25pt'>
  <p class=MsoNormal align=center style='text-align:center'><b><span
  style='font-size:8.5pt;color:black'>Sec<o:p></o:p></span></b></p>
  </td>
  <td style='background:#E7E7E7;padding:2.25pt 2.25pt 2.25pt 2.25pt'>
  <p class=MsoNormal align=center style='text-align:center'><b><span
  style='font-size:8.5pt;color:black'>Unt<o:p></o:p></span></b></p>
  </td>
  <td style='background:#E7E7E7;padding:2.25pt 2.25pt 2.25pt 2.25pt'>
  <p class=MsoNormal align=center style='text-align:center'><b><span
  style='font-size:8.5pt;color:black'>Instructor<o:p></o:p></span></b></p>
  </td>
  <td style='background:#E7E7E7;padding:2.25pt 2.25pt 2.25pt 2.25pt'>
  <p class=MsoNormal align=center style='text-align:center'><b><span
  style='font-size:8.5pt;color:black'>Time<o:p></o:p></span></b></p>
  </td>
  <td style='background:#E7E7E7;padding:2.25pt 2.25pt 2.25pt 2.25pt'>
  <p class=MsoNormal align=center style='text-align:center'><b><span
  style='font-size:8.5pt;color:black'>Place<o:p></o:p></span></b></p>
  </td>
 </tr>
 <tr style='mso-yfti-irow:1;mso-yfti-lastrow:yes'>
  <td nowrap valign=top style='background:#D5E5FF;padding:2.25pt 2.25pt 2.25pt 2.25pt'>
  <p class=MsoNormal><span style='font-size:8.5pt;color:black'>36745<o:p></o:p></span></p>
  </td>
  <td nowrap valign=top style='background:#FFFFCC;padding:2.25pt 2.25pt 2.25pt 2.25pt'>
  <p class=MsoNormal><span style='font-size:8.5pt;color:black'>Lec<o:p></o:p></span></p>
  </td>
  <td nowrap valign=top style='background:#D5E5FF;padding:2.25pt 2.25pt 2.25pt 2.25pt'>
  <p class=MsoNormal><span style='font-size:8.5pt;color:black'>A<o:p></o:p></span></p>
  </td>
  <td nowrap valign=top style='background:#FFFFCC;padding:2.25pt 2.25pt 2.25pt 2.25pt'>
  <p class=MsoNormal><span style='font-size:8.5pt;color:black'>4<o:p></o:p></span></p>
  </td>
  <td nowrap valign=top style='background:#D5E5FF;padding:2.25pt 2.25pt 2.25pt 2.25pt'>
  <p class=MsoNormal><span style='font-size:8.5pt;color:black'>WELLING, M.<o:p></o:p></span></p>
  </td>
  <td nowrap valign=top style='background:#FFFFCC;padding:2.25pt 2.25pt 2.25pt 2.25pt'>
  <p class=MsoNormal><span style='font-size:8.5pt;color:black'>TuTh &nbsp;
  2:00- 3:20p<o:p></o:p></span></p>
  </td>
  <td nowrap valign=top style='background:#D5E5FF;padding:2.25pt 2.25pt 2.25pt 2.25pt'>
  <p class=MsoNormal><span style='font-size:8.5pt;color:black'>CS 213<o:p></o:p></span></p>
  </td>
 </tr>
</table>

<h2><span style='font-size:10.0pt'>

<hr size=2 width="100%" align=left>

</span></h2>

<h2><span style='font-size:10.0pt;font-family:Arial;color:red'>Prerequisites</span><span
style='font-size:10.0pt'><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt'>ICS 274A<span
style='mso-spacerun:yes'>&nbsp; </span>Probabilistic Learning: Theory<br>
and Algorithms, or with consent of instructor.<o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt'>

<hr size=2 width="100%" align=left>

</span></h2>

<h2><span style='font-size:10.0pt;font-family:Arial;color:red'>Goals</span><span
style='font-size:10.0pt'><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt;color:black'>Many modern approaches to
probabilistic modeling of real world<br>
data sets can be formulated in the unifying framework of graphical<br>
models. Graphical models provide a common language to think and<br>
communicate about probabilistic models and makes explicit the<br>
underlying assumptions. Moreover, it provides the appropriate<br>
structure for computations necessary for inference and learning in<br>
these models.</span><span style='font-size:10.0pt'><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt;color:black'>It is the primary goal of this
course to familiarize the student<br>
with the concepts of graphical models, and in particular with<br>
learning these models from data. A student who has successfully<br>
completed the course should be able to understand a wide variety<br>
of well known models in terms of this unifying framework and feel<br>
comfortable using it to design new models. The course will contain<br>
1) formal mathematical sections necessary for the development of<br>
the theory, 2) examples of probabilistic models (re)formulated in<br>
the language of graphical models and 3) examples of successful<br>
applications to real data.<o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt'>This secondary goal of this class is to give
the <br>
students hands on experience in solving real world problems.<br>
For that purpose I have negotiated a deal with SciTech, a San Diego based
company:<br>
if we improve their (naive Bayes) classifier on a particular classification
problem <br>
(prediction of activity levels of chemical compounds - for data, see below)
then they <br>
will provide a $300 bonus for the student who will come down and present this
work.<br>
In addition, provided their goals are met one student can implement this
algorithm into <br>
their software package as a summer intern.<o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt;font-family:Arial;color:black'>

<hr size=2 width="100%" align=left>

</span></h2>

<h2><span style='font-size:10.0pt;font-family:Arial;color:red'>Homework : </span><span
style='font-size:10.0pt;font-family:Arial;color:black'>(slides serve to give
you an impression what was done last time, but I expect<br>
<span
style='mso-spacerun:yes'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span>that we will significantly deviate from that. Also, homework will be
updated as we go.)<o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt;font-family:Arial;color:red'>Book: </span><span
style='font-size:10.0pt;font-family:Arial'>Book-chapters can be found in this <a
href="Jordan_book">password protected directory</a></span><span
style='font-size:8.0pt'><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt;color:blue'>week 1:</span><span
style='font-size:10.0pt;color:red'> </span><span style='font-size:10.0pt'><a
href="ROC.ppt">ROC</a><br>
<span style='color:black'>- read sections 2.1, 2.2, 2.3 from </span><a
href="22.40.pdf">chapter 2</a><span style='color:black'> of David MacKay's
book.</span><br>
<span style='color:black'>- read chapter 2, 5 (until &quot;plates&quot;) &amp;
13 from Mike Jordan's book.</span><br>
<span style='color:black'>- read </span><a href="Estimation.pdf">classnotes</a><br>
<span style='color:black'>- </span><a href="HOMEWORK/HW1.PDF">Excercises HW1</a><span
style='color:black'> </span><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt;color:blue'>week 2:</span><span
style='font-size:10.0pt;color:red'> </span><span style='font-size:10.0pt'><br>
<span style='color:black'>- read chapter 6, 7 from Mike Jordan's book.</span><br>
<span style='color:black'>- </span><a href="HOMEWORK/HW2.pdf">Excercises HW2</a><span
style='color:black'> </span></span><span style='font-size:8.0pt;color:black'>(only
the relevant ones on topics we have treated in class)</span><span
style='font-size:10.0pt;color:black'><br>
- <a href="HOMEWORK/Project%201.doc">Project 1</a> (due May 4)<o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt'>&nbsp;<span style='color:blue'>week 3: </span><br>
<span style='color:black'>- read chapter 9, 19, 20 from Mike Jordan's book.</span><br>
<span style='color:black'>- </span><a href="HOMEWORK/HW3.pdf">Excercises HW3</a><span
style='color:black'> , <span style='mso-spacerun:yes'>&nbsp;</span></span><a
href="HOMEWORK/HW4.pdf">Excercises HW4</a> <span style='color:black'><span
style='mso-spacerun:yes'>&nbsp;</span><br>
</span></span><span style='font-size:8.0pt;color:black'>(only the relevant ones
on topics we have treated in class. At this point homework is optional but
instructive. )</span><span style='font-size:10.0pt;color:black'><br
style='mso-special-character:line-break'>
<![if !supportLineBreakNewLine]><br style='mso-special-character:line-break'>
<![endif]><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt;color:black'>--------------------------------------------------------------------------------------<br>
</span><span style='font-size:10.0pt;color:red'>(stuff below this line is not
updated)</span><span style='font-size:10.0pt;color:black'><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt;color:blue'>week 4: </span><span
style='font-size:10.0pt'><br>
<span style='color:black'>- read chapter 8 from Mike Jordan's book (only those
topics treated in class).</span><br style='mso-special-character:line-break'>
<![if !supportLineBreakNewLine]><br style='mso-special-character:line-break'>
<![endif]><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt'>&nbsp;<span style='color:blue'>week 5:</span>
overview(9)<span style='color:black'> </span><br>
<span style='color:black'>- read chapter 10, 11 from Mike Jordan's book (only
those topics treated in class).</span><br>
<span style='color:black'>- read </span><a href="EM.pdf">classnotes</a><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt'>&nbsp;<span style='color:blue'>week 6: </span><a
href="ICS280_VEM.ppt">1</a><span style='color:red'> </span><br>
<span style='color:black'>- </span>Excercises HW6<span style='color:black'>
(due Th. Feb.26.)</span><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt'>&nbsp;<span style='color:blue'>week 7</span><span
style='color:black'>: </span><a href="PCA.ps">classnotes</a><br>
<span style='color:black'>-</span><span style='color:blue'> </span><span
style='color:black'>read chapter 14 and classnotes</span><br>
<span style='color:black'>- work on projects</span><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt'>&nbsp;<span style='color:blue'>week 8: </span><a
href="Approx.ps">classnotes</a><br>
<span style='color:black'>-read chapter 1 and classnotes (only topics explained
in class are required reading material)</span><br>
<span style='color:black'>-work on projects</span><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt'>&nbsp;<span style='color:blue'>week 9:</span><a
href="ICS280_HMM.ppt">slides 16</a><span style='color:black'> </span><a
href="ICS280_KF.ppt">slides 17</a><span style='color:black'> </span><a
href="HMM.ps">classnotes(HMM)</a><span style='color:black'> </span><a
href="KalmanFilter.ps">classnotes(KF)</a><br>
<span style='color:black'>-read chapters 12, 15, classnotes and the tutorial (only
topics explained in class are required reading material).</span><br>
<span style='color:black'>-work on projects</span><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt;color:blue'>week 10:</span><span
style='font-size:10.0pt'><br>
<span style='color:black'>- presentation projects:</span> <br>
<span style='color:black'>- review of current trends in machine learning</span><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt'>&nbsp;<span style='color:blue'>week 11:</span><span
style='color:black'> </span><span style='color:red'>final exam:</span><span
style='color:black'> </span><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt'>&nbsp;<span style='color:blue'><o:p></o:p></span></span></h2>

<h2><span style='font-size:10.0pt;color:blue'>

<hr size=2 width="100%" align=left>

</span></h2>

<h2><span style='font-size:10.0pt;font-family:Arial;color:red'>MATLAB Demos:</span><span
style='font-size:10.0pt'><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt;color:blue'>week 1: </span><span
style='font-size:10.0pt'><a href="demo_Bayes.m">demo_Bayes</a><span
style='color:black'>, </span><a href="demo_MAP.m">demo_MAP</a><span
style='color:black'>, </span><a href="demo_ML.m">demo_ML</a><span
style='color:black'>, </span><a href="plotGauss1D.m">plotGauss1D</a><span
style='color:black'>, </span><a href="plotGauss.m">plotGauss2D, </a><a
href="ginput2.m">ginput2</a><br>
<span style='color:blue'>week 2:</span><span style='color:black'> </span><a
href="demo_LinReg.m">demo_LinReg</a><span style='color:black'>, </span><a
href="demo_LogReg.m">demo_LogReg</a><br>
<span style='color:blue'>week5:</span><a href="demo_EM.m"> demo_EM</a><br>
<span style='color:blue'>week6: </span><a href="MoG_demo.m">MoG_demo</a><span
style='color:blue'>, </span><a href="plotGauss_color.m">plotGauss_color</a><span
style='color:blue'>, </span><a href="randMean.m">randMean</a><span
style='color:blue'>, </span><a href="randCovariance.m">randCovariance</a><span
style='color:blue'>, </span><a href="kmeans.m">kmeans</a><span
style='color:blue'>, </span><a href="dist2.m">dist2</a><span style='color:blue'>,
</span><a href="randvec.m">randvec</a><span style='color:blue'>, </span><a
href="gaussian.m">gaussian</a><br>
<span style='color:blue'>week7:</span> <a href="demo_pca.m">demo_pca</a>, <a
href="FA.m">FA</a><br>
<span style='color:blue'>week8: </span><a href="demo_gibbs.m">demo_gibbs</a>, <a
href="demo_mcmc.m">demo_mcmc</a><br>
<span style='color:blue'>week9: </span><a href="HMM_demo4.m">demo_HMM </a><a
href="demo_KF.m">demo_KF</a>, <a href="demo_KF2.m">demo_KF2</a><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt;font-family:Arial;color:black'>

<hr size=2 width="100%" align=left>

</span></h2>

<h2><span style='font-size:10.0pt;font-family:Arial;color:red'>SciTech Dataset:<o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt;color:#000099'><a href="train_label.mat">Training
labels</a>: </span><span style='font-size:10.0pt'>0: inactive compound, 1:
medium active compound, 2: active compound.<o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt;color:#000099'><a href="train_att_cont.mat">Continuous
attributes</a>: </span><span style='font-size:10.0pt'>2 continuous attributes:
AlogP and Molecular_Weight<o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt;color:#000099'><a href="train_att_discr.mat">Discrete
attributes</a>: </span><span style='font-size:10.0pt'>3 discrete attributes:
Num_H_Acceptors,<span style='mso-spacerun:yes'>&nbsp; </span>Num_H_Donors,
Num_RotatableBonds.<o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt;color:#000099'><a href="train_att_bin.mat">Binary
finger print</a>: </span><span style='font-size:10.0pt'>very sparse binary
matrix where <span style='color:#000099'><span
style='mso-spacerun:yes'>&nbsp;</span></span>1&#8217;s code for the presence of
certain substructures.<o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt'>Using all the available attributes we wish
to predict the activity level of the compound.<br>
Background <st1:place w:st="on"><st1:City w:st="on">Reading</st1:City></st1:place>:
<a href="David%20Rogers%20-%20ECFP%20Manuscript.doc">paper1</a>,<span
style='mso-spacerun:yes'>&nbsp; </span><a href="Amgen-Bayesian.pdf">paper2</a>,<span
style='mso-spacerun:yes'>&nbsp; </span><a href="NaiveBayesian.ppt">SciTech
powerpoint slides</a>.<o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt;font-family:Arial;color:black'>

<hr size=2 width="100%" align=left>

</span></h2>

<h2><span style='font-size:10.0pt;font-family:Arial;color:red'>Syllabus:</span><span
style='font-size:10.0pt'><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt'>The course will primarily be lecture-based
with homework and<br>
exams. Most homework will revolve around the implementation of various<br>
classification algorithms on the SciTech dataset provided above.<br>
It is required that you use MATLAB for this coding work. <o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt'>The following is a rough syllabus subject to
change.<o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt;color:#000099'>1. Review of Statistical
Concepts</span><span style='font-size:10.0pt'><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt'>Random variables, probability distributions
and<br>
probability densities. The multivariate Gaussian distribution.<br>
Marginal and conditional independence. Bayes' rule. Estimation:<br>
maximum likelihood, MAP-estimates, Bayesian inference,<br>
bias-variance tradeoff. Model selection and averaging,<br>
over-fitting.<o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt;color:#000099'>2. Graphical Models.</span><span
style='font-size:10.0pt'><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt'>Markov random fields and undirected<br>
graphical models. Bayesian networks and directed acyclic graphical<br>
models. Semantics of graphical models: independence assumptions,<br>
Markov properties, Markov blanket, separability. Factor graphs,<br>
chain graphs. Plates.<o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt;color:#000099'>3. Hidden Variables and Exact
Inference.</span><span style='font-size:10.0pt'><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt'>Observed and hidden random variables. Bayes'
ball algorithm. Exact inference:<br>
junction tree propagation and cut-set conditioning.<o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt;color:#000099'>4. Learning in Graphical
Models.</span><span style='font-size:10.0pt'><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt'>The expectation<br>
maximization algorithm and free energy minimization. Iterative<br>
conditional modes. Iterative scaling.<o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt;color:#000099'>5. Unsupervised Learning -
Directed Graphical Models.</span><span style='font-size:10.0pt'><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt'>Mixture of Gaussians, K-means, principal components
analysis,<br>
probabilistic principal components analysis, factor analysis,<br>
independent components analysis, latent Dirichlet allocation.<o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt;color:#000099'>6. Unsupervised Learning -
Undirected Graphical Models.</span><span style='font-size:10.0pt'><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt'>Boltzmann machines, products of experts,
additive random field<br>
models. Examples in vision and text.<o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt;color:#000099'>7. Supervised Learning -
Directed and Undirected Graphical Models.</span><span style='font-size:10.0pt'><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt'>Naive Bayes as a graphical model,<br>
logistic regression, linear regression.<br>
Conditional mixture models, mixtures of experts. Conditional<br>
random fields.<o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt;color:#000099'>8. Graphical Models of Time
Series.</span><span style='font-size:10.0pt'><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt'>State space models, autoregressive models.<br>
Hidden Markov Models. The Baum-Welch and Viterbi algorithm. The<br>
Kalman filter and smoother. Dynamic Bayes nets. Examples in speech<br>
and biological sequence data.<o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt;color:#000099'>9. Approximate Inference.</span><span
style='font-size:10.0pt'><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt'>Mean field methods and<br>
structured variational inference. Loopy belief propagation. Region<br>
graphs and generalized belief propagation. Sampling: rejection<br>
sampling, importance sampling, particle filters, Markov chain<br>
Monte Carlo sampling, Gibbs sampling, Hybrid <st1:place w:st="on">Monte Carlo</st1:place>
sampling.<o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt;color:#000099'>10. Bayesian Learning and
Structure Learning in Graphical Models.</span><span style='font-size:10.0pt'><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt'>Conjugate priors.<br>
Fully observed Bayes' nets. Variational Bayes algorithm. Sampling<br>
from the posterior. <st1:place w:st="on">Laplace</st1:place> approximation.
Chow-Liu's algorithm<br>
for trees. Structure learning in fully observed Bayes' nets.<br>
Structure learning in the presence of hidden variables: structural<br>
EM.<o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt'>

<hr size=2 width="100%" align=left>

</span></h2>

<h2><span style='font-size:10.0pt;font-family:Arial;color:red'>Grading Criteria</span><span
style='font-size:10.0pt'><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt;color:black'><br>
Grading will be based on a combination of weekly homework and a project (40%<br>
of the grade), a midterm exam (30%) and a final exam (30%) .</span><span
style='font-size:10.0pt'><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt;color:black'>

<hr size=2 width="100%" align=left>

</span></h2>

<h2><span style='font-size:10.0pt;font-family:Arial;color:red'>Textbook</span><span
style='font-size:10.0pt'><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt;color:#000099'><br>
The textbook that will be used for this course has not been<br>
published yet, but copies will distributed during class.</span><span
style='font-size:10.0pt'><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt;color:black'>1. M.I. Jordan: An Introduction
to Graphical Models.</span><span style='font-size:10.0pt'><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt;color:#000099'><br>
Optional side readings are:</span><span style='font-size:10.0pt'><o:p></o:p></span></h2>

<h2><span style='font-size:10.0pt;color:black'>2. D. MacKay: Information
Theory, Inference and Learning<br>
Algorithms<br>
3. M.I. Jordan: Learning in Graphical Models<br>
4. B. Frey: Graphical Models for Machine Learning and Digital<br>
Communication<br>
5. J. Pearl: Probabilistic Reasoning in Intelligent<br>
Systems<br>
6. R.O. Duda, P.E. Hart, D. Stork: Pattern<br>
Classification<br>
7. C.M. Bishop: Neural Networks for Pattern Recognition<br>
8. T. Hastie, R. Tibshirani, J.H, Friedman: The Elements of<br>
Statistical Learning<br>
9. B.D. Ripley: Pattern Recognition and Neural Networks</span></h2>

</div>

</body>

</html>

