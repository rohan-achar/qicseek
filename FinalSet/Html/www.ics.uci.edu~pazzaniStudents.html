<HTML>
<head>
<title> Michael J. Pazzani: Students and Affiliated Researchers </title>
</head>

<body BGCOLOR="#FFFFFF">
<h1>  Michael J. Pazzani: Research Group </h1>


<h2>Graduate Students</h2>
<ul>


</ul>

<h2>Ph.D. Graduates</h2>
<li><a href=http://www.ics.uci.edu/~sbay>Stephen Bay</a>
<Li><a href =http://www.ics.uci.edu/~dbillsus>Daniel Billsus</a>
<li><a href=http://www.ics.uci.edu/~eamonn>Eamonn Keogh</a>

<ul> 
<li><a href ="http://www.ics.uci.edu/~wogulis"><b>James Wogulis</b></a> <a
href="wogulis.ps">  An Approach to Repairing and Evaluating
First-Order Theories Containing Multiple Concepts and Negation.</a> (1.2M)
This dissertation addresses the problem of theory revision in machine
learning.  The task requires the learner to minimally revise an
initial incorrect theory such that the revised theory explains a given
set of training data. A learning system, A3, is presented that solves
this task.  The main contributions of this dissertation include the
learning system A3 that can revise theories containing multiple
concepts expressed as function-free first-order Horn clauses, an
approach to repairing theories containing negation, and the
introduction of a distance metric between theories to evaluate the
degree of revision performed.  Experimental evidence is presented that
demonstrates A3's ability to solve the theory revision task.
Assumptions commonly made by other approaches to theory revision such
as whether a theory needs to be generalized or specialized with
respect to misclassified examples are shown to be incorrect for
theories containing negation.  A3 is able to repair theories
containing negation and demonstrates a simple, general approach to
identifying types of errors in a theory using a single mechanism for
handling positive and negative examples as well as examples of
multiple concepts.  The syntactic distance between two theories is
proposed as an evaluation metric for theory revision systems. This
distance is defined in terms of the minimum number of edit operations
required to transform one theory into another. This allows for a
precise measurement of how much a theory has been revised and allows
for comparison of different systems' abilities to perform minimal
revisions.  This distance metric is also used by A3 in order to bias
it towards finding minimal revisions that accurately explain the data.
The distance metric also leads to insights about the theory revision
task. In particular, it is shown that the theory revision task is
underconstrained if the additional goal of learning a particular
correct theory is to be met. Without additional constraints, there are
potentially many accurate revisions that are far apart syntactically.
It is shown that providing examples of multiple concepts in the theory
can provide some of these constraints.

<p><li> <a href= "http://www.ics.uci.edu/~schulenb"><b>David
Schulenburg</b></a> <a href="schulenb.ps">Learning and Using Context
in a connectionist Model of Language Understanding</a> (2.5M) Natural
languages are ambiguous.  This is especially true for non-literal
figurative constructs such as metaphors and indirect speech acts.
Even literal text suffers from problems of ambiguity as exemplified by
text containing words having multiple meanings.  Understanding such
ambiguous text is a fairly simple task for us humans; it is well
recognized that context is often used by people to aid in the
resolution of these problems of ambiguity.  This dissertation presents
a discussion of a computational model, PIP, which was designed to
address the issue of ambiguity in natural language understanding.  By
incorporating a textual context during the understanding process, PIP
is able to disambiguate text containing ambiguous constructs such as
metaphors and indirect speech acts.  Furthermore, the entire
understanding process is uniform in the sense that the exact same
mechanisms are used to process both literal and non-literal
(ambiguous) text; no special processing ``rules'' are necessary to
deal with the ambiguity.  The underlying computational model of PIP is
the feed-forward artificial neural network.  The incorporation of the
textual context is accomplished by a recurrent relation between the
context that is being constructed and the network input for processing
the words of the text.  In this fashion is PIP able to use the context
directly as it processes text.  PIP has demonstrated its effectiveness
on sets of text which include ambiguous lexemes, metaphors, and
indirect speech acts.  By using the context constructed from earlier
sentences in these texts, PIP is able to derive the intended meaning
of the ambiguous sentences at the end of the texts.  It is shown that
without use of the context, PIP is unable to produce the intended
meaning and in many cases, cannot decide on any meaning to give to the
ambiguous sentences.

<p><li> <a href= "http://www.isle.org/~ali"><b> Kamal Ali</b></a> <a href="ali.ps">Learning Probabilistic Relational Concept Descriptions</a> (1.6M) 
This dissertation presents results in the area of multiple models
(multiple classifiers), learning probabilistic relational (first order)
rules from noisy, "real-world" data and reducing  the small disjuncts
problem - the problem whereby learned rules that cover few training examples
have high error rates on test data.
<p>
Several results are also presented in the arena of multiple models.  The
multiple models approach in relevant to the problem of making accurate
classifications in ``real-world'' domains since it facilitates evidence
combination which is needed to accurately learn on such domains.
It is also useful when learning from small training data samples in which 
many models appear to be equally "good" w.r.t. the given evaluation metric.
Such models often have quite varying error rates on test data so in such
situations, the single model method has problems. Increasing search only
partly addresses this problem whereas the multiple models approach has the
potential to be much more useful.

The most important result of the multiple models research is that the
*amount* of error reduction afforded by the multiple models approach is
linearly correlated with the degree to which the individual models make
errors in an uncorrelated manner. This work is the first to model the degree
of error reduction due to the use of multiple models.  It is also shown that
it is possible to learn models that make less correlated errors in domains
in which there are many ties in the search evaluation metric during
learning.  The third major result of the research 
on multiple models is the realization that models should be learned that
make errors in a negatively-correlated manner rather than those that make
errors in an uncorrelated (statistically independent) manner.

The thesis also presents results on learning probabilistic first-order rules
from relational data.  It is shown that learning a class description for
each class in the data - the one-per-class approach - and attaching
probabilistic estimates to the learned rules allows accurate classifications
to be made on real-world data sets.  The thesis presents the system HYDRA
which implements this approach.  It is shown that the resulting
classifications are often more accurate than those made by three existing
methods for learning from noisy, relational data.  Furthermore, the learned
rules are relational and so are more expressive than the attribute-value
rules learned by most induction systems.
<p>
Finally, results are presented on the small-disjuncts problem in which rules
that apply to rare subclasses have high error rates
The thesis presents the first approach that is simultaneously successful
at reducing the error rates of small disjucnts while also reducing the
overall error rate by a statistically significant margin. The previous
approach which aimed to reduce small disjunct error rates only did so at the
expense of increasing the error rates of large disjuncts.
It is shown that the one-per-class approach reduces error rates for such
rare rules while not sacrificing the error rates of the other rules.

<p><li> <a href= "http://www.ics.uci.edu/~brunk"><b>Cliff Brunk</b></a>
<a href="brunk.ps">An Investigation of Knowledge Intensive Approaches to 
Concept Learning and Theory Refinement</a> (1.6M) Concept learning algorithms have been used to solve difficult problems
in fields ranging from medical diagnosis to astronomy.  In spite of
their successful application, most concept learners are only able to
utilize knowledge that is expressed in the form of a set of training
examples.  Relevant knowledge from other sources can not be utilized
even when it is available.  Evidence is presented that using knowledge
from other sources leads to more accurate learned models.
This dissertation is an investigation of techniques for utilizing
knowledge in the form of an approximate theory to facilitate concept
learning.  The techniques explored are divided into two classes:
theory-guided learning algorithms which use the approximate theory to
constrain the search for a new concept description, and theory
revision algorithms which attempt to repair the approximate theory.
While both techniques are more accurate than approaches that ignore
the information contained in the approximate theory, experimental
evidence indicates that theory revision algorithms produce more
accurate models.  Furthermore, these models are structurally more
similar to both the approximate theory and the "ideal" theory, than
those produced by theory-guided learning algorithms.
The main contributions of this dissertation include: a new approach to
theory-guided learning, a conceptual framework for comparing and
evaluating theory revision algorithms, enhanced techniques for
identifying and repairing errors within a theory, and a lexically
enhanced approach to evaluating repairs. Lexically enhanced theory
revision is a novel technique for utilizing a previously unused form
of knowledge contained in the approximate theory.  The technique uses
lexical information contained in the term names of the approximate
theory to prefer repairs that are lexically more coherent.  Evidence
indicates that this further reduces the structural difference between
the revised theory and the "ideal" theory.

<p><li> <a href= "http://www.ics.uci.edu/~cmerz"><b>Christopher Merz</b></a>
<a href="merz.ps">Classification and Regression by Combining Models</a> 
Two novel methods for combining predictors are introduced in this 
thesis; one for the task of regression, and the other for the task of 
classification.  The goal of combining the predictions of a set of 
models is to form an improved predictor.  This dissertation 
demonstrates how a combining scheme can rely on the stability of the 
consensus opinion and, at the same time, capitalize on the unique 
contributions of each model.
<p>
An empirical evaluation reveals that the new methods 
consistently perform as well or better than existing combining schemes 
for a variety of prediction problems.  The success of these algorithms is 
explained empirically and analytically by demonstrating how they 
adhere to a set of theoretical and heuristic guidelines.
<p>
A byproduct of the empirical investigation is the evidence that 
existing combining methods fail to satisfy one or more of the 
guidelines defined.  The new combining approaches satisfy these 
criteria by relying upon Singular Value Decomposition as a tool for 
filtering out the redundancy and noise in the predictions of the learn 
models, and for characterizing the areas of the example space where 
each model is superior.  The SVD-based representation used in the new 
combining methods aids in avoiding sensitivity to correlated 
predictions without discarding any learned models.  Therefore, the 
unique contributions of each model can still be discovered and 
exploited.  An added advantage of the combining algorithms derived in 
this dissertation is that they are not limited to models generated by 
a single algorithm; they may be applied to model sets generated by a 
diverse collection of machine learning and statistical 
modeling methods.
<p>
The three main contributions of this dissertation are:
<ol><li>The introduction of two new combining methods capable of 
robustly combining classification and regression estimates, and 
applicable to a broad range of model sets.
<li> An in-depth analysis revealing how the new methods address the 
specific problems encountered in combining multiple learned models.
<li> A detailed account of existing combining methods and an 
assessment of where they fall short in the criteria for combining 
approaches.
</ol>

</ul>



<HR>

<P>

<ADDRESS>
<A HREF="http://www.ics.uci.edu/~pazzani">Michael J. Pazzani</A><br>
<A HREF="http://www.ics.uci.edu/">Department of Information and Computer Science,</A><br>
<A HREF="http://www.uci.edu/">University of California, Irvine</A><br>
Irvine, CA 92697-3425 <br>
<A href="mailto:pazzani@ics.uci.edu">pazzani@ics.uci.edu </A>
</ADDRESS>
</BODY></HTML>

