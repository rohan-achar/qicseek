<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN">

<html>
<head>
  <meta name="generator" content=
  "HTML Tidy for Linux/x86 (vers 7 December 2008), see www.w3.org">
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8">

  <title>UCI ISG Lecture Series on Scalable Data Management</title>
  <style type="text/css">
/*<![CDATA[*/
      body{background:white;color:#404040;font-family:Georgia,serif;margin:60px;}
      h1{color:#22222;font-size:280%;vertical-align:text-top;}
      td{padding:4px;vertical-align:text-top;}
      div.details{border:1px solid grey;padding:4px;}
      div.float{float:left;}
      div.clear{clear:both;}
      div.title{width:70%;}
      div.logo{margin:20px 10px;}
      a img{border-width:0px;}
      td.c2{font-weight: bold}
      td.c1{font-weight: bold; width: 100px}
  /*]]>*/
  </style>
</head>

<body>
  <div class="float logo">
    <a href="http://isg.ics.uci.edu/"><img src= "images/isg-200px-nourl.png"
    width="200px" alt="UCI ISG Logo"></a>
  </div>

  <div class="float title">
    <h1>UCI ISG Lecture Series on Scalable Data Management</h1>
  </div>

  <div class="clear"></div>

  <div id="container">
    <div class="content">
      <table>
        <colgroup span="1" width="130"></colgroup>

        <colgroup span="1" width="*"></colgroup>

        <tr>
          <td>
            <div class="eventDate">
              Oct 2, 2009
            </div>
          </td>

          <td>
            <div class="eventInfor">
              <b>Teradata Past, Present and Future</b><br>
              Todd Walter, CTO, R&amp;D, Teradata
            </div>
          </td>
        </tr>

        <tr>
          <td></td>

          <td><a href="#" onclick=
          'var x=getElementById("e70");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>
          Details</a> <a href="slides/Teradata.ppt">Slides</a>
          <a href="videos/Teradata.mov">Video</a> (512MB)</td>
        </tr>

        <tr>
          <td colspan="2">
            <div class="details" style="display:none;" id="e70">
              <table>
                <tr>
                  <td class="c1">Date and Time</td>

                  <td>Oct 2, 2009 2pm</td>
                </tr>

                <tr>
                  <td class="c2">Location</td>

                  <td>DBH 4011</td>
                </tr>
              </table><br>

              <table>
                <tr>
                  <td class="c1">Speaker</td>

                  <td>Todd Walter, Teradata</td>
                </tr>

                <tr>
                  <td class="c2">Title</td>

                  <td>Teradata Past, Present and Future</td>
                </tr>

                <tr>
                  <td class="c2">Abstract</td>

                  <td>In the early 1980's, Teradata Corporation
                  introduced the world's first commercial
                  shared-nothing parallel database management
                  system. Today, Teradata manages many of the
                  world's largest data warehouses. A look at
                  Teradata's past will show how parallelism was
                  applied over a wide variety of generations of
                  hardware and business challenges. Then we will
                  look at the key differentiators of the product as
                  they exist today and where the product may go in
                  the future. Customer examples will be used
                  throughout.</td>
                </tr>

                <tr>
                  <td class="c2">Speaker Bio</td>

                  <td>Todd Walter joined Teradata in 1987. He
                  designed and implemented features of Teradata,
                  managed engineering teams and researched advanced
                  database topics. Todd has worked directly with
                  many customers at the leading edge of adopting
                  data warehouse technology, guiding the technology
                  to solve real business problems. He holds several
                  Teradata patents and in 1998 was named Teradata
                  Fellow, the highest technical award granted by
                  the company. In his current role as Chief
                  Technical Officer of Teradata R&amp;D, Todd is
                  responsible for vision, strategy and technical
                  leadership, taking Teradata into the future.</td>
                </tr>
              </table>
            </div>
          </td>
        </tr>

        <tr>
          <td>
            <div class="eventDate">
              Oct 9, 2009
            </div>
          </td>

          <td>
            <div class="eventInfor">
              <b>Hadoop: Origins and Applications</b><br>
              Chris Smith, Xavier Stevens and John Carnahan, FOX
              Audience Network
            </div>
          </td>
        </tr>

        <tr>
          <td></td>

          <td><a href="#" onclick=
          'var x=getElementById("e71");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>
          Details</a> <a href="slides/FAN.pptx">Slides</a> <a href=
          "videos/FAN.mov">Video</a> (348MB)</td>
        </tr>

        <tr>
          <td colspan="2">
            <div class="details" style="display:none;" id="e71">
              <table>
                <tr>
                  <td class="c1">Date and Time</td>

                  <td>Oct 9, 2009 2pm</td>
                </tr>

                <tr>
                  <td class="c2">Location</td>

                  <td>DBH 4011</td>
                </tr>
              </table><br>

              <table>
                <tr>
                  <td class="c1">Speaker</td>

                  <td>Chris Smith, Xavier Stevens and John Carnahan,
                  FOX Audience Network</td>
                </tr>

                <tr>
                  <td class="c2">Title</td>

                  <td>Hadoop: Origins and Applications</td>
                </tr>

                <tr>
                  <td class="c2">Abstract</td>

                  <td>Hadoop has become a powerful tool for facile,
                  cost-effective parallel processing of massive
                  quantities of data. FAN (Fox Audience Network)
                  employs Hadoop to rapidly build and execute
                  innovative statistical models and perform
                  extensive analyses on massive quantities of data.
                  This talk will first outline the history of
                  Hadoop and of the Map/Reduce framework which
                  inspired it. It will then describe the Hadoop
                  execution model and distributed filesystem.
                  Finally, it will discuss practical applications
                  of Hadoop, illustrated by specific examples from
                  FAN.</td>
                </tr>
              </table>
            </div>
          </td>
        </tr>

        <tr>
          <td>
            <div class="eventDate">
              Oct 16, 2009
            </div>
          </td>

          <td>
            <div class="eventInfor">
              <b>Pig: Building High-Level Dataflows over
              Map-Reduce</b><br>
              Utkarsh Srivastava, Senior Research Scientist, Yahoo!
            </div>
          </td>
        </tr>

        <tr>
          <td></td>

          <td><a href="#" onclick=
          'var x=getElementById("e72");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>
          Details</a> <a href="slides/YahooPig.ppt">Slides</a>
          <a href="videos/YahooPig.mov">Video</a> (270MB)</td>
        </tr>

        <tr>
          <td colspan="2">
            <div class="details" style="display:none;" id="e72">
              <table>
                <tr>
                  <td class="c1">Date and Time</td>

                  <td>Oct 16, 2009 2pm</td>
                </tr>

                <tr>
                  <td class="c2">Location</td>

                  <td>DBH 4011</td>
                </tr>
              </table><br>

              <table>
                <tr>
                  <td class="c1">Speaker</td>

                  <td>Utkarsh Srivastava, Senior Research
                  Scientist, Yahoo!</td>
                </tr>

                <tr>
                  <td class="c2">Title</td>

                  <td>Pig: Building High-Level Dataflows over
                  Map-Reduce</td>
                </tr>

                <tr>
                  <td class="c2">Abstract</td>

                  <td>Internet companies routinely capture and
                  analyze large data sets, using the obtained
                  insights to guide product design. For such large
                  data analysis, programmers have flocked to the
                  Map-Reduce programming model that is simple,
                  scalable, and at the same time, extremely
                  versatile. The explicit dataflow programming
                  style of Map-Reduce is preferred by many over the
                  high-level declarative style of SQL. However, the
                  map-reduce paradigm is too low-level and rigid,
                  leaving the users to write a lot of custom code
                  for common operations such as filter, join etc,
                  and for dealing with multi-step and branching
                  dataflows. Such custom code is hard to reuse and
                  maintain, and also impedes optimizations. I will
                  talk about the design and implementation of Pig:
                  a high-level dataflow system that aims at a
                  sweet-spot between the declarative style of SQL
                  and the procedural style of Map-Reduce. Pig
                  offers SQL-style high-level data
                  manipulation constructs, which can be assembled
                  in an explicit dataflow and interleaved with
                  custom Map- and Reduce-style functions or
                  executables. Pig programs are compiled into
                  sequences of Map-Reduce jobs, and executed in the
                  Hadoop Map-Reduce environment. Both Pig and
                  Hadoop are open-source projects administered by
                  the Apache Software Foundation.</td>
                </tr>
              </table>
            </div>
          </td>
        </tr>

        <tr>
          <td>
            <div class="eventDate">
              Oct 23, 2009
            </div>
          </td>

          <td>
            <div class="eventInfor">
              <b>Database Scalability and Indexes</b><br>
              Goetz Graefe, HP Fellow, Hewlett-Packard Laboratories
            </div>
          </td>
        </tr>

        <tr>
          <td></td>

          <td><a href="#" onclick=
          'var x=getElementById("e73");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>
          Details</a> <a href="slides/HP.ppt">Slides</a> <a href=
          "videos/HP.mov">Video</a> (455MB)</td>
        </tr>

        <tr>
          <td colspan="2">
            <div class="details" style="display:none;" id="e73">
              <table>
                <tr>
                  <td class="c1">Date and Time</td>

                  <td>Oct 23, 2009 2pm</td>
                </tr>

                <tr>
                  <td class="c2">Location</td>

                  <td>DBH 4011</td>
                </tr>
              </table><br>

              <table>
                <tr>
                  <td class="c1">Speaker</td>

                  <td>Goetz Graefe, HP Fellow, Hewlett-Packard
                  Laboratories</td>
                </tr>

                <tr>
                  <td class="c2">Title</td>

                  <td>Database Scalability and Indexes</td>
                </tr>

                <tr>
                  <td class="c2">Abstract</td>

                  <td>Scalability can be achieved by brute-force
                  parallelism or by smart data structures and
                  algorithms. This presentation and discussion will
                  focus on data structures and algorithms that
                  enable scalability and energy efficiency on
                  systems of any size.</td>
                </tr>
              </table>
            </div>
          </td>
        </tr>

        <tr>
          <td>
            <div class="eventDate">
              Nov 6, 2009
            </div>
          </td>

          <td>
            <div class="eventInfor">
              <b>Cloud Data Serving: Key-Value Stores to
              DBMSs</b><br>
              Raghu Ramakrishnan, Chief Scientist for Audience
              &amp; Cloud Computing, Yahoo!
            </div>
          </td>
        </tr>

        <tr>
          <td></td>

          <td><a href="#" onclick=
          'var x=getElementById("e75");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>
          Details</a> <a href="slides/Yahoo.ppt">Slides</a>
          <a href="videos/Yahoo.mov">Video</a> (395MB)</td>
        </tr>

        <tr>
          <td colspan="2">
            <div class="details" style="display:none;" id="e75">
              <table>
                <tr>
                  <td class="c1">Date and Time</td>

                  <td>Nov 6, 2009 11am (not regular ISG seminar
                  time!)</td>
                </tr>

                <tr>
                  <td class="c2">Location</td>

                  <td>DBH 6011 (not regular ISG seminar
                  location!)</td>
                </tr>
              </table><br>

              <table>
                <tr>
                  <td class="c1">Speaker</td>

                  <td>Raghu Ramakrishnan, Chief Scientist for
                  Audience &amp; Cloud Computing, Yahoo!</td>
                </tr>

                <tr>
                  <td class="c2">Title</td>

                  <td>Cloud Data Serving: Key-Value Stores to
                  DBMSs</td>
                </tr>

                <tr>
                  <td class="c2">Abstract</td>

                  <td>Data-backed web applications have stringent
                  availability, performance and partition tolerance
                  requirements that are difficult, sometimes even
                  impossible, to meet using conventional database
                  management systems. On the other hand, they
                  typically are able to trade off consistency to
                  achieve their goals. This has led to the
                  development of specialized key-value stores,
                  which are now used widely in virtually every
                  large-scale web service. On the other hand, most
                  web services also require capabilities such as
                  indexing provided by a DBMS. We are witnessing an
                  evolution of data serving as systems builders
                  seek to balance these trade-offs. In this talk, I
                  will survey some of the solutions that have been
                  developed, including Amazon's S3 and SimpleDB and
                  Yahoo!'s PNUTS, and discuss the challenges in
                  building such systems as "cloud services",
                  providing elastic data serving capacity to
                  developers, along with appropriately balanced
                  consistency, availability, performance and
                  partition tolerance.</td>
                </tr>

                <tr>
                  <td class="c2">Speaker Bio</td>

                  <td>Raghu Ramakrishnan is Chief Scientist for
                  Audience and Cloud Computing at Yahoo!, and is a
                  Research Fellow, heading the Web Information
                  Management group. His work has influenced query
                  optimization in commercial database systems and
                  the design of window functions in SQL:1999. His
                  paper on the Birch clustering algorithm received
                  the SIGMOD 10-Year Test-of-Time award, and he has
                  written the widely-used text "Database Management
                  Systems" (with Johannes Gehrke). Ramakrishnan is
                  a Fellow of the ACM and IEEE, and has received
                  several awards, including the ACM SIGKDD
                  Innovations Award, the ACM SIGMOD Contributions
                  Award, a Distinguished Alumnus Award from IIT
                  Madras, a Packard Foundation Fellowship in
                  Science and Engineering, and an NSF Presidential
                  Young Investigator Award. He is Chair of ACM
                  SIGMOD, on the Board of Directors of ACM SIGKDD
                  and the Board of Trustees of the VLDB Endowment.
                  Ramakrishnan was Professor of Computer Sciences
                  at the University of Wisconsin-Madison, and
                  founder and CTO of QUIQ, a company that pioneered
                  question-answering communities, powering Ask
                  Jeeves' AnswerPoint as well as customer-support
                  for companies such as Compaq.</td>
                </tr>
              </table>
            </div>
          </td>
        </tr>

        <tr>
          <td>
            <div class="eventDate">
              Nov 13, 2009
            </div>
          </td>

          <td>
            <div class="eventInfor">
              <b>Scalable Data Management at Facebook</b><br>
              Srinvas Narayanan, Software Engineer, Facebook
            </div>
          </td>
        </tr>

        <tr>
          <td></td>

          <td><a href="#" onclick=
          'var x=getElementById("e76");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>
          Details</a> <a href="slides/Facebook.pptx">Slides</a>
          <a href="videos/Facebook.mov">Video</a> (210MB)</td>
        </tr>

        <tr>
          <td colspan="2">
            <div class="details" style="display:none;" id="e76">
              <table>
                <tr>
                  <td class="c1">Date and Time</td>

                  <td>Nov 13, 2009 2pm</td>
                </tr>

                <tr>
                  <td class="c2">Location</td>

                  <td>DBH 4011</td>
                </tr>
              </table><br>

              <table>
                <tr>
                  <td class="c1">Speaker</td>

                  <td>Srinvas Narayanan, Software Engineer,
                  Facebook</td>
                </tr>

                <tr>
                  <td class="c2">Title</td>

                  <td>Scalable Data Management at Facebook</td>
                </tr>

                <tr>
                  <td class="c2">Abstract</td>

                  <td>Facebook is one of the most trafficked
                  websites with over 300 million active users
                  generating hundreds of billions of page views a
                  month. In this talk, I’ll present an overview of
                  the data infrastructure that powers Facebook.
                  I’ll cover aspects of the live MySQL + memcached
                  based infrastructure as well as the offline
                  Hive/Hadoop based infrastructure. I’ll also talk
                  about lessons learnt in running the site at this
                  large scale, and discuss some interesting open
                  problems, challenges and opportunities.</td>
                </tr>
              </table>
            </div>
          </td>
        </tr>

        <tr>
          <td>
            <div class="eventDate">
              Nov 20, 2009
            </div>
          </td>

          <td>
            <div class="eventInfor">
              <b>SCOPE: Parallel Data Processing of Massive Data
              Sets</b><br>
              Jingren Zhou, Researcher, Microsoft
            </div>
          </td>
        </tr>

        <tr>
          <td></td>

          <td><a href="#" onclick=
          'var x=getElementById("e77");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>
          Details</a> <a href=
          "slides/MicrosoftSCOPE.pptx">Slides</a> <a href=
          "videos/MicrosoftSCOPE.mov">Video</a> (328MB)</td>
        </tr>

        <tr>
          <td colspan="2">
            <div class="details" style="display:none;" id="e77">
              <table>
                <tr>
                  <td class="c1">Date and Time</td>

                  <td>Nov 20, 2009 2pm</td>
                </tr>

                <tr>
                  <td class="c2">Location</td>

                  <td>DBH 4011</td>
                </tr>
              </table><br>

              <table>
                <tr>
                  <td class="c1">Speaker</td>

                  <td>Jingren Zhou, Researcher, Microsoft</td>
                </tr>

                <tr>
                  <td class="c2">Title</td>

                  <td>SCOPE: Parallel Data Processing of Massive
                  Data Sets</td>
                </tr>

                <tr>
                  <td class="c2">Abstract</td>

                  <td>Companies providing cloud-scale services have
                  an increasing need to store and analyze massive
                  data sets such as search logs and click streams.
                  For cost and performance reasons, processing is
                  typically done on large clusters of
                  shared-nothing commodity machines. It is
                  imperative to develop a programming model that
                  hides the complexity of the underlying system but
                  provides flexibility by allowing users to extend
                  functionality to meet a variety of requirements.
                  In this talk, we present a new declarative and
                  extensible scripting language, SCOPE (Structured
                  Computations Optimized for Parallel Execution),
                  targeted for this type of massive data analysis
                  at Microsoft. The language is designed for ease
                  of use with no explicit parallelism, while being
                  amenable to efficient parallel execution on large
                  clusters. SCOPE borrows several features from
                  SQL. Data is modeled as sets of rows composed of
                  typed columns. The select statement is retained
                  with inner joins, outer joins, and aggregation
                  allowed. Users can easily define their own
                  functions and implement their own versions of
                  operators: extractors (parsing and constructing
                  rows from a file), processors (row-wise
                  processing), reducers (group-wise processing),
                  and combiners (combining rows from two inputs).
                  SCOPE supports nesting of expressions but also
                  allows a computation to be specified as a series
                  of steps, in a manner often preferred by
                  programmers. We also describe how scripts are
                  compiled and optimized into efficient, parallel
                  execution plans and executed on large
                  clusters.</td>
                </tr>
              </table>
            </div>
          </td>
        </tr>

        <tr>
          <td>
            <div class="eventDate">
              Dec 4, 2009
            </div>
          </td>

          <td>
            <div class="eventInfor">
              <b>What We Got Right, What We Got Wrong: The Lessons
              I Learned Building a Large-Scale DBMS for
              XML.</b><br>
              Mary Holstege, Principal Engineer, Mark Logic
            </div>
          </td>
        </tr>

        <tr>
          <td></td>

          <td><a href="#" onclick=
          'var x=getElementById("e78");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>
          Details</a> <a href="slides/MarkLogic.ppt">Slides</a>
          <a href="videos/MarkLogic.mov">Video</a> (289MB)</td>
        </tr>

        <tr>
          <td colspan="2">
            <div class="details" style="display:none;" id="e78">
              <table>
                <tr>
                  <td class="c1">Date and Time</td>

                  <td>Dec 4, 2009 2pm</td>
                </tr>

                <tr>
                  <td class="c2">Location</td>

                  <td>DBH 4011</td>
                </tr>
              </table><br>

              <table>
                <tr>
                  <td class="c1">Speaker</td>

                  <td>Mary Holstege, Principal Engineer, Mark Logic</td>
                </tr>

                <tr>
                  <td class="c2">Title</td>

                  <td>What We Got Right, What We Got Wrong: The
                  Lessons I Learned Building a Large-Scale DBMS for
                  XML.</td>
                </tr>
              </table>
            </div>
          </td>
        </tr>

        <tr>
          <td>
            <div class="eventDate">
              Dec 11, 2009
            </div>
          </td>

          <td>
            <div class="eventInfor">
              <b>Scalable Data Management with DB2</b><br>
              Matthias Nicola, DB2 pureXML Architect, IBM
            </div>
          </td>
        </tr>

        <tr>
          <td></td>

          <td><a href="#" onclick=
          'var x=getElementById("e79");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>
          Details</a> <a href="slides/Scalable Data Management with DB2.pdf">Slides</a> <a href=
          "videos/IBM.mov">Video</a> (394MB)</td>
        </tr>

        <tr>
          <td colspan="2">
            <div class="details" style="display:none;" id="e79">
              <table>
                <tr>
                  <td class="c1">Date and Time</td>

                  <td>Dec 11, 2009 2pm</td>
                </tr>

                <tr>
                  <td class="c2">Location</td>

                  <td>DBH 4011</td>
                </tr>
              </table><br>

              <table>
                <tr>
                  <td class="c1">Speaker</td>

                  <td>Matthias Nicola, DB2 pureXML Architect,
                  IBM</td>
                </tr>

                <tr>
                  <td class="c2">Title</td>

                  <td>Scalable Data Management with DB2</td>
                </tr>

                <tr>
                  <td class="c2">Abstract</td>

                  <td>Enterprises around the world are relying on
                  DB2 for scalable data management such as
                  transaction processing applications and data
                  warehousing. This presentation provides an
                  overview of two scalability concepts that DB2
                  offers: the DB2 database partitioning feature
                  (DPF) and DB2 pureScale. DB2's database
                  partitioning feature provides users with a
                  parallel database system in a shared-nothing
                  architecture. More computing nodes can be added
                  to the DPF system to meet growing demands in
                  terms of data volume and workload. We will
                  discuss the data partitioning and placement
                  options as well as query processing concepts in a
                  DPF database. We will also examine the setup and
                  results of a recent scalability benchmark for
                  complex XML query processing in a DB2 partitioned
                  database. Contrary to the warehousing focus of
                  DPF, the DB2 pureScale technology provides a
                  parallel and scalable solution for online
                  transaction processing applications. We provide
                  an overview of the shared-disk architecture and
                  transaction processing concepts in DB2
                  pureScale.</td>
                </tr>

                <tr>
                  <td class="c2">Speaker Bio</td>

                  <td>Matthias Nicola is a senior engineer for DB2
                  pureXML at IBM's Silicon Valley Lab. His work
                  focuses on all aspects of XML in DB2, including
                  XQuery, SQL/XML, XML storage, indexing and
                  performance. Matthias also works closely with
                  customers and business partners, assisting them
                  in the design, implementation, and optimization
                  of XML solutions. He is a co-author of the /DB2
                  pureXML Cookbook/. Prior to joining IBM, Matthias
                  worked on data warehousing performance for
                  Informix Software, focusing on query
                  optimization, star join processing, high-speed
                  data loading, and other features. In 1999,
                  Matthias received a Ph.D. in computer science
                  from the Technical University of Aachen, Germamy.
                  (http://www.matthiasnicola.de/)</td>
                </tr>
              </table>
            </div>
          </td>
        </tr>

        <tr>
          <td>
            <div class="eventDate">
              Jan 8, 2010
            </div>
          </td>

          <td>
            <div class="eventInfor">
              <b>SQL Server: A Data Platform for Large-Scale
              Applications</b><br>
              José Blakeley, Partner Architect, Microsoft
            </div>
          </td>
        </tr>

        <tr>
          <td></td>

          <td><a href="#" onclick=
          'var x=getElementById("e80");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>
          Details</a> <a href=
          "slides/MicrosoftSQLServer.pptx">Slides</a> <a href=
          "videos/MicrosoftSQLServer.mov">Video</a> (324MB)</td>
        </tr>

        <tr>
          <td colspan="2">
            <div class="details" style="display:none;" id="e80">
              <table>
                <tr>
                  <td class="c1">Date and Time</td>

                  <td>Jan 8, 2010 2pm</td>
                </tr>

                <tr>
                  <td class="c2">Location</td>

                  <td>DBH 4011</td>
                </tr>
              </table><br>

              <table>
                <tr>
                  <td class="c1">Speaker</td>

                  <td>José Blakeley, Partner Architect, Microsoft</td>
                </tr>

                <tr>
                  <td class="c2">Title</td>

                  <td>SQL Server: A Data Platform for Large-Scale
                  Applications</td>
                </tr>

                <tr>
                  <td class="c2">Abstract</td>

                  <td>We describe proven data management
                  architectures, design patterns and practices
                  being used by Microsoft's largest-scale customers
                  who service millions of users every day. The
                  applications span the range from online
                  transaction processing to data warehousing
                  workloads. We present database technologies and
                  hardware architectures that best address the
                  needs of the most demanding large-scale
                  data-driven applications.</td>
                </tr>

                <tr>
                  <td class="c2">Speaker Bio</td>

                  <td>José Blakeley is Partner Architect in the SQL
                  Server Division at Microsoft where he works on
                  server programmability, database engine
                  extensibility, query processing,
                  object-relational functionality, large scale
                  query processing, and scientific database
                  applications. He joined Microsoft in 1994. Some
                  of his contributions include the development of
                  the OLE DB data access interfaces, the
                  integration of the .NET runtime inside the SQL
                  Server 2005, the extensibility features in SQL
                  Server, and the creation of the ADO.NET Entity
                  Framework in Visual Studio 2008. José has
                  authored many conference papers, book chapters
                  and journal articles on design aspects of
                  relational and object database management
                  systems, and data access. Before joining
                  Microsoft, José was a member of the technical
                  staff with Texas Instruments where he was
                  co-principal investigator of the DARPA Open-OODB
                  system. He received a B. Eng from ITESM,
                  Monterrey, Mexico, and a Ph.D. in computer
                  science from University of Waterloo, Canada.</td>
                </tr>
              </table>
            </div>
          </td>
        </tr>

        <tr>
          <td>
            <div class="eventDate">
              May 7, 2010
            </div>
          </td>

          <td>
            <div class="eventInfor">
              <b>Data in the Cloud: New Challenges or More of the
              Same?</b><br>Divy Agrawal, Professor of Computer
              Science, UC Santa Barbara
            </div>
          </td>
        </tr>

        <tr>
          <td></td>

          <td><a href="#" onclick=
          'var x=getElementById("e81");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>
          Details</a> <a href=
          "slides/Divy.pptx">Slides</a> <a href=
          "videos/Divy.mov">Video</a> (402 MB)</td>
        </tr>

        <tr>
          <td colspan="2">
            <div class="details" style="display:none;" id="e81">
              <table>
                <tr>
                  <td class="c1">Date and Time</td>

                  <td>May 7, 2010 2pm</td>
                </tr>

                <tr>
                  <td class="c2">Location</td>

                  <td>DBH 4011</td>
                </tr>
              </table><br>

              <table>
                <tr>
                  <td class="c1">Speaker</td>

                  <td>Divy Agrawal, Professor of Computer Science, UC
                  Santa Barbara</td>
                </tr>

                <tr>
                  <td class="c2">Title</td>

                  <td>Data in the Cloud: New Challenges or More of the
                  Same?</td>
                </tr>

                <tr>
                  <td class="c2">Abstract</td>

                  <td>Over the past two decades, database and systems
                  researchers have made significant advances in the
                  development of algorithms and techniques to provide
                  data management solutions that carefully balance the
                  three major requirements when dealing with critical
                  data: high availability, scalability, and data
                  consistency. However, over the past few years the
                  data requirements, in terms of availability and
                  scalability, from Internet scale enterprises that
                  provide services and cater to millions of users has
                  been unprecedented. Current proposed solutions to
                  scalable data management, driven primarily by
                  prevalent application requirements, significantly
                  downplay the data consistency requirements and
                  instead focus on very high availability and almost
                  unlimited scalability to support data-rich
                  applications for millions to tens of millions of
                  users. In particular, the "newer" data management
                  systems limit consistent access only at the
                  granularity of single objects, rows, or keys,
                  thereby significantly trading-off consistency in
                  order to achieve very high scalability and
                  availability. But the growing popularity of "cloud
                  computing", the resulting shift of a large number of
                  Internet applications to the cloud, and the quest
                  towards providing data management services in the
                  cloud, has opened up the challenge for designing
                  data management systems that provide consistency
                  guarantees at a granularity which goes beyond single
                  rows and keys. In this talk, we analyze the design
                  choices that allowed modern scalable data management
                  systems to achieve orders of magnitude higher levels
                  of scalability compared to traditional
                  databases. With this understanding, we highlight
                  some design principles for data management systems
                  providing scalable and consistent data management as
                  a service in the cloud. We conclude the talk by
                  presenting results from two prototype systems which
                  strike a middle-ground between the two radically
                  different data management architectures: traditional
                  database management systems where the data is
                  treated as a "whole" versus modern key-value stores
                  where data is treated as a collection of independent
                  "granules".</td>
                </tr>

                <tr>
                  <td class="c2">Speaker Bio</td>

                  <td>Dr. Divy Agrawal serves on the faculty of
                  Computer Science at the University of California at
                  Santa Barbara. His research interests are in the
                  areas of distributed systems, databases, and
                  large-scale information systems such as data
                  warehouses, digital libraries, and other
                  data/information rich environments.</td>
                </tr>
              </table>
            </div>
          </td>
        </tr>

	<tr>
          <td>
            <div class="eventDate">
              Nov 12, 2010
            </div>
          </td>

          <td>
            <div class="eventInfor">
              <b>Programming and Debugging Large-Scale Data Processing Workflows</b><br>Chris Olston, Research Scientist, Yahoo
            </div>
          </td>
        </tr>

        <tr>
          <td></td>

          <td><a href="#" onclick=
          'var x=getElementById("e82");if(x.style.display!="none"){x.style.display="none";}else{x.style.display="block";}return false;'>
          Details</a> <a href=
          "slides/data-prog-debug.pptx">Slides</a> <a href=
          "videos/Yahoo_olston.mp4">Video</a> (402 MB)</td>
        </tr>

        <tr>
          <td colspan="2">
            <div class="details" style="display:none;" id="e82">
              <table>
                <tr>
                  <td class="c1">Date and Time</td>

                  <td>Nov 12, 2010 3pm</td>
                </tr>

                <tr>
                  <td class="c2">Location</td>

                  <td>DBH 3011</td>
                </tr>
              </table><br>

              <table>
                <tr>
                  <td class="c1">Speaker</td>

                  <td> Chris Olston, Research Scientist, Yahoo!</td>
                </tr>

                <tr>
                  <td class="c2">Title</td>

                  <td>Programming and Debugging Large-Scale Data Processing Workflows</td>
                </tr>

                <tr>
                  <td class="c2">Abstract</td>

                  <td>This talk gives an overview of some work on large-scale data processing I have done with my Yahoo collaborators. The first half describes two data processing systems I helped develop: PIG, a dataflow programming environment and Hadoop-based runtime, and NOVA, a workflow manager for Pig/Hadoop. The second half focuses on debugging, and looks at what can be done before, during and after execution of a data processing operation:
  * Pig's automatic EXAMPLE DATA GENERATOR is used before running a Pig job to get a feel for what it will do, enabling certain kinds of mistakes to be caught early and cheaply. The algorithm behind the example generator performs a combination of sampling and synthesis to balance several key factors---realism, conciseness and completeness---of the example data it produces.
  * INSPECTOR GADGET is a framework for creating custom tools that monitor Pig job execution. We have implemented a dozen user-requested tools, ranging from data integrity checks to crash cause investigation to performance profiling, each in just a few hundreds of lines of code.
  * IBIS is a system that collects metadata about what happened during data processing, for post-hoc analysis. The metadata is collected from multiple sub-systems (e.g. Nova, Pig, Hadoop) that deal with data and processing elements at different granularities (e.g. tables vs. records; relational operators vs. reduce task attempts) and offer disparate ways of querying it. IBIS integrates this metadata and presents a uniform and powerful query interface to users.</td>
                </tr>

                <tr>
                  <td class="c2">Speaker Bio</td>

                  <td>Christopher Olston is a principal research scientist at Yahoo! Research, focusing on data management and web search problems. Olston won the 2009 SIGMOD Best Paper Award, and co-created Apache Pig, which is now widely adopted at Yahoo, LinkedIn, Twitter and other companies, and has been incorporated into Amazon's EC2 Elastic Map-Reduce offering. Olston is occasionally seen behaving as a professor, and has taught undergrad and grad courses at Berkeley, Carnegie Mellon and Stanford. He received his Ph.D. in 2003 from Stanford under fellowships from the university and the National Science Foundation. His Bachelor's degree is from Berkeley with highest honors. Olston is an avid Cal fan but likes to rollerblade at Stanford.</td>
                </tr>
              </table>
            </div>
          </td>
        </tr>

      </table>
    </div>

    <div class="content">
      <p>For more information on <a href=
      "http://isg.ics.uci.edu/">ISG</a> lectures, please visit
      <a href="http://isg.ics.uci.edu/events.html">ISG Events</a>.
      For more information on CS distinguished lectures, please
      visit <a href=
      "http://www.ics.uci.edu/computerscience/research/seminarseries/">
      Computer Science Department Seminar Series</a>.</p>

      <p>Support for the ISG Seminar Series from Yahoo! is
      gratefully acknowledged.</p>
      <hr>

      <p>For any questions about this page, please contact <a href=
      "http://www.ics.uci.edu/~rares/">Rares Vernica.</a></p>
    </div>
  </div>
</body>
</html>

