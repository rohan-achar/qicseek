<!DOCTYPE html 
    PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" 
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html>
<head>
  <title>SLI | Classes-CS178-Notes / GmmEM </title>
  <meta http-equiv='Content-Style-Type' content='text/css' />
  <link rel='stylesheet' href='http://sli.ics.uci.edu/pmwiki/pub/skins/custom/pmwiki.css' type='text/css' />
  <!--HTMLHeader--><style type='text/css'><!--
  ul, ol, pre, dl, p { margin-top:0px; margin-bottom:0px; }
  code.escaped { white-space: nowrap; }
  .vspace { margin-top:1.33em; }
  .indent { margin-left:40px; }
  .outdent { margin-left:40px; text-indent:-40px; }
  a.createlinktext { text-decoration:none; border-bottom:1px dotted gray; }
  a.createlink { text-decoration:none; position:relative; top:-0.5em;
    font-weight:bold; font-size:smaller; border-bottom:none; }
  img { border:0px; }
  .editconflict { color:green; 
  font-style:italic; margin-top:1.33em; margin-bottom:1.33em; }

  table.markup { border:2px dotted #ccf; width:90%; }
  td.markup1, td.markup2 { padding-left:10px; padding-right:10px; }
  table.vert td.markup1 { border-bottom:1px solid #ccf; }
  table.horiz td.markup1 { width:23em; border-right:1px solid #ccf; }
  table.markup caption { text-align:left; }
  div.faq p, div.faq pre { margin-left:2em; }
  div.faq p.question { margin:1em 0 0.75em 0; font-weight:bold; }
  div.faqtoc div.faq * { display:none; }
  div.faqtoc div.faq p.question 
    { display:block; font-weight:normal; margin:0.5em 0 0.5em 20px; line-height:normal; }
  div.faqtoc div.faq p.question * { display:inline; }
   
    .frame 
      { border:1px solid #cccccc; padding:4px; background-color:#f9f9f9; }
    .lfloat { float:left; margin-right:0.5em; }
    .rfloat { float:right; margin-left:0.5em; }
a.varlink { text-decoration:none; }

--></style>
  <link href='http://sli.ics.uci.edu/pmwiki/pub}/commentboxplus/commentboxplus.css' rel='stylesheet' type='text/css' />  <meta name='robots' content='index,follow' />

</head>
<body>
<!--PageHeaderFmt-->
  <div id='wikilogo'><a href='http://sli.ics.uci.edu'><img src='/pmwiki/pub/skins/custom/SLI_white.png'
    alt='SLI' border='0' /></a></div>
  <div id='wikihead'>
  <form action='http://sli.ics.uci.edu'>
    <!-- <span class='headnav'><a href='http://sli.ics.uci.edu/Classes-CS178-Notes/RecentChanges'
      accesskey='c'>Recent Changes</a> -</span> --> 
    <input type='hidden' name='n' value='Classes-CS178-Notes.GmmEM' />
    <input type='hidden' name='action' value='search' />
    <!-- <a href='http://sli.ics.uci.edu/Site/Search'>Search</a>: -->
    <input type='text' name='q' value='' class='inputbox searchbox' />
    <input type='submit' class='inputbutton searchbutton'
      value='Search' />
    <a href='http://sli.ics.uci.edu/Site/Search'>(?)</a>
  </form></div>
<!--/PageHeaderFmt-->
  <table id='wikimid' width='100%' cellspacing='0' cellpadding='0'><tr>
<!--PageLeftFmt-->
      <td id='wikileft' valign='top'>
        <ul><li><a class='wikilink' href='http://sli.ics.uci.edu/Classes/Classes'>Classes</a>
</li><li><a class='wikilink' href='http://sli.ics.uci.edu/Group/Group'>Group</a>
</li><li><a class='wikilink' href='http://sli.ics.uci.edu/Projects/Projects'>Research</a>
</li><li><a class='urllink' href='http://www.ics.uci.edu/~ihler/pubs.html' title='' rel='nofollow'>Publications</a>
</li><li><a class='wikilink' href='http://sli.ics.uci.edu/Code/Code'>Code</a>
</li></ul><div class='vspace'></div><hr />
<div class='vspace'></div>
</td>
<!--/PageLeftFmt-->
      <td id='wikibody' valign='top'>
<!--PageActionFmt-->
        <div id='wikicmds'><ul><li class='browse'><a class='wikilink' href='http://sli.ics.uci.edu/Classes-CS178-Notes/GmmEM?action=login'>login</a>
</li></ul>
</div>
<!--PageTitleFmt-->
        <div id='wikititle'>
          <div class='pagegroup'><a href='http://sli.ics.uci.edu/Classes-CS178-Notes'>Classes-CS178-Notes</a> /</div>
          <h1 class='pagetitle'>GmmEM</h1></div>
<!--PageText-->
<div id='wikitext'>
<p>Gaussian mixture models are perhaps the most common model used for explaining (or modeling) data using clustering ideas.
</p>
<p class='vspace'>Although standard Gaussian models are common in many situations, they are not appropriate for many problems.  For example, it is common to suppose that grades in a course should be Gaussian distributed (given enough students, of course).  However, suppose that our data consist of grades for <strong>two</strong> courses, lumped together.  If the courses have very different level of difficulty, the resulting distribution of grades will certainly not look Gaussian -- for example, the histogram might show two modes, centered at the averages within each course.  How might we model such data, that consist of a mixture of two (or more) different populations?
</p>
<p class='vspace'>Mixture models address this task.  Instead of using a single Gaussian distribution, we can define a weighted sum of Gaussian components (for example, one per course).  In this case, we have a set of Gaussian parameters (mean <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/bccca90bf9da55b6eaa548cbca621c80.png" /> and covariance <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/7533e2b2b67eeffe5f6165dec2a0bbc9.png" />) for each component, and a "mixture weight" $\pi_c$ that describes the relative proportion of data from group c in the full data.  Then, the overall distribution is a weighted average of these components:
</p><img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/ca4b4ec8afaa4dff46c908af85b0320f.png" /> 
<p>where <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/6fd15f5d5ce6738097ce05e62ef3a2f2.png" /> is the standard Gaussian distribution,
</p><img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/0531e2d809e376bb7d99b3257f4b167e.png" /> 
<p class='vspace'>The log-likelihood of the data under this model is given by
</p><img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/62f660ba0151e6bb6f022e140c3fce27.png" /> 
<p>As is typical in probabilistic models, we would like to optimize the log-likelihood over the model parameters <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/a1069a19fe463be22c32624c9b8d275f.png" />, so that the data are explained as well as possible by the model.  However, this optimization cannot be solved in closed form, in part due to the "log sum exp" form taken on by the log of a mixture of Gaussians.  One possibility, of course, is to optimize it directly via gradient ascent.  However, an augmented form will give us other possibilities, and make the connection to other clustering algorithms more clear.
</p>
<div class='vspace'></div><h2>GMM as Clustering</h2>
<p>As in the above example, we typically do not know which data come from each component.  In fact, if we knew which data were which, we could more easily model the groups individually.  Alternatively, if we knew the component models, we might be able to determine which component each datum came from.  Thus identifying both the assignment of data to each component, and the "description" of the component (<img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/75c63b560de5308bdc82e52ef391f683.png" />) can be seen to be a variant of the classic formulation of clustering.
</p>
<p class='vspace'>As in K-means, let us associate a variable <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/a0c0d4deb0683a7aa5e9b8761b6d767f.png" /> with each data point <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/feb9550eef79cb34ef24e0c2764c3bb0.png" /> to indicate which component gave rise to this particular observation.  If we were <strong>told</strong> this association, the data within each component would simply be Gaussian, and the association variables z would tell us <em>'which</em> Gaussian component to evaluate.  We can use this to write down the so-called "complete" log-likelihood of the data:
</p><img class='TrueLatexImage' style='vertical-align:middle;' border=0 src="http://sli.ics.uci.edu/pmwiki/pub/latexcache/1087bce216f373e40771815911e5e059.png" /> 
<p class='vspace'>Expect cll
</p>
<p class='vspace'>,
</p>
<p class='vspace'>Optim  and variat interp
Lower bound with q
</p>
</div>

      </td>
    </tr></table>
<!--PageFooterFmt-->
  <div id='wikifoot'>
    <div class='footnav' style='float:left'> Last modified February 26, 2012, at 05:59 PM</div>
    <div class='footnav' style='float:right; text-align:right'>
    <a href="http://www.ics.uci.edu">Bren School of Information and Computer Science</a><br>
    <a href="http://www.uci.edu">University of California, Irvine</a>
    </div>
  </div>
<!--HTMLFooter--><script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(["_setAccount", "UA-24148957-2"]);
	_gaq.push(["_trackPageview"]);
	(function() {
	  var ga = document.createElement("script"); ga.type = "text/javascript"; ga.async = true;
	  ga.src = ("https:" == document.location.protocol ? "https://ssl" : "http://www") + ".google-analytics.com/ga.js";
	  var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(ga, s);
	  })();
</script>
</body>
</html>

