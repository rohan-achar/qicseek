<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>

<HEAD>
    <META HTTP-EQUIV="Content-Type" CONTENT="text/html;CHARSET=iso-8859-1">
    <META NAME="GENERATOR" Content="Visual Page 2.0 for Windows">
    <TITLE>untitled</TITLE>
</HEAD>

<BODY BGCOLOR="#CCFFFF">

<P>Roughly the course will require that you read and understand 2 chapters per week. Some sections of the chapters
will not be required. You are expected to learn the assigned reading material even if it is not covered in the class.
lectures. The pace of the course will vary with the difficulty of the material.</P>
<P>There are three coding projects, dealing with problem solving, logical reasoning, and learning.</P>
<P>The first assignment has been updated and is correct.</P>
<P><BR>
<B>Homework Grading: </B>The homeworks are deliberately open-ended. If you do a minimal job, your grade will be
a B. If you do more and it is mostly correctly, then the grade is A What counts here is the intelligence/thought
that you add to the assignment. If less, then C.</P>
<P><BR>
<B>Week 1.</B> Read Chapters 1, 2(summary only), and 3. Introduction to AI. State-Space Representations and Search. DFS, BFS,
IDS, Uniform-Cost. Bidirectional search.</P>
<b>Lecture 1. </b><a HREF="270ALecture01.ppt"> AI Intro</a></p>
<b> Lecture 2. </b><a HREF="StateSpace.ppt"> State-Space Representation</a>
<P><B>Week 2.</B> Read Chapter 4 (not 4.3) Informed Search and more: Best First, A*, Beam, Stochastic, Iterative
Improvement. Generating heuristics. Simulated annealing.</P>
<b>Lecture 3. </b><a HREF="UnInformedSearch.ppt"> UnInformed Search </a> </p>
<b>Lecture 4&5. </b> <a href="InformedSearch.ppt"> Informed Search </a> <p>

<P><B>Week 3.</B> <a href="ConstraintSatisfaction.ppt"> Constraint Satisfaction</a> </P>

<P><B>Week 4.</B> <a href="GamePlaying.ppt"> Game Playing </a> and <a href="PropositionalLogic.ppt">Propositional Logic</a>
</P> Your first homework is due at class time.

<P><B>Week 5.</B> Read Chapters 8  &amp; 9 Propositional Logic and  <a href="Prolog.ppt"> Prolog. </a> </P>

<P><B>Week 6.</B>  <a href="PredicateLogic.ppt"> First-Order Logic </a>  </P>

<P><B>Week 7.</B> Read Chapters 14 &amp; 15. <a href="Probability.ppt">Probabilistic Reasoning</a></P>

<P><B>Week 8.</B> Read Chapters 18 &amp; <a href="BayesNetworks.ppt" > Bayesian Networks </a>  <a href="Weka.ppt"> 
Weka Suite of ML algorithms  </a> <a href="LearningOverview.ppt"> 
Learning in Pictures </a>
</P>

<P><B>Week 9.</B> Read Chapter 18.  <a href="DecisionTrees.ppt"> Decision Trees.</a>

<P><B>Week 10.</B> Read Chapter 18 &amp; 20. <a href="LinearDiscriminators.ppt"> Linear Discriminators</a>
 and <a href="IBL.ppt"> Nearest Neighbor or Instance-Based Learning </a>

<P><BR>
<BR>
<B>Homework 1</B>. Search. Due at the beginning of the fourth week. Due Date is October 21. In this problem you will evaluate several different
search algorithms. The problem to be solved is the traveling salesman problem. Each city has two integer co-ordinates and every city is connected to every other city by a straight-line edge. The search algorithms that you need to code are:
<p> a) A non-informed exhaustive search algorithm such as depth first, breadth first, or iterative deepening. Hint: because of the repeated states, I suggest that you just define a function/method that generates all permutations of the cities.

<P>b) The A* algorithm with a suitable heuristic function. CHANGE: it is extra credit to code this algorithm because of the problem with repeated states. You still need to define a non-trivial admissible heuristic h.
<p>c) The simulated annealing algorithm.
<p> d)The iterative improvement algorithm (also called local improvement) with
 the &quot;uncross&quot; heuristic. Uncross will be explained in class.</P>
<P> You may be asked to turn in your code or demonstrate your algorithm. What you need to turn in is an evaluation of the algorithms.  Here is a minimal evaluation.

<P>a) For ten problems of size 10, compare the performance of your algorithms, in terms of speed and quality
of solution or any other measure that you can think of. If problems of size 10 take too long, you may choose a slightly smaller number.</P>

<P>b) What is the largest problem (number of cities) that you can "solve" by every algorithm in less than 1 minute?</P>

<P> Your typed write-up should be as brief as possible, but no briefer. Your conclusions should be supported by the experiments that you have run, i.e. the data from the experiments that you discuss should be included in your write-up. You may assume I know the algorithms. However you should explain the operators (which may change with the algorithm) and the heuristic you've chosen for A*. You should explain your evaluation of the algorithms. At the end you should  discuss the results. 

<P><BR>
<B>Homework 2.</B> Logic. Due at the beginning of the seventh week. Because of the holiday on Nov 11, this is due on Nov 13. This assignment requires the use Prolog. 
<ol>
  <li> Do problem 7.8 page 237 in the new edition. (Propositional Logic) Just the answers.
  <li> Do problem 8.6 page 269 in the edition. Write the FOL form.
  <li>
 This problem is less structured. You are to write Prolog code for the micro-world of spatial reasoning. At a
minimum you should include predicates: at(X,Y), in(X,Y), between(X,Y,Z), location(X), object(Y), on(X,Y). What
types of reasoning does your micro-world support? What types of reasoning within this domain does it not support?</P>
<P>Here's an example: ( you should do 5-10 different inferences)</P>
<P>% John has a picture of his wife in his wallet. John is at the spectrum. Inference: picture is at the Spectrum,
i.e. the answer  to the question at(picture, X) binds X to spectrum.</P>
<P>Now provide the Prolog code to do it.</P>
You should provide your code and trace outputs that demonstrate the capability of your program. 
</ol>
<P><BR>
<B><BR>
Homework 3.</B> Learning. Due at the beginning of the tenth week.
<ol>
 <li> Do problems 13.6, 13.8, 13,10 from chapter 13 (new edition).
 <li> Do problem 18.7 from chapter 18 (new edition).
 <li> Download the Weka software (I suggest 3.2.3 but you could try the newer version) onto your computer and prepare the dermatology dataset from the UCI 
machine learning database(http;// www.ics.uci.edu/~mlearn)
 for analysis by the Weka learning programs. From the Weka suite of algorithms use decision trees (j48), rule learning (j48.part),
 nearest neighbor( IB1 or IBK) and Naive Bayes on the data. Report how you evaluated the algorithms, which
 worked best, whether you were surprised and anything you thought was interesting.
</ol>

</BODY>

</HTML> 
